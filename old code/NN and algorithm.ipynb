{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ee0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00b837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d12d57c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load your CSV data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotic_radiant_fog_160_dev.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace \"your_dataset.csv\" with the actual file path\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Replace \"category\" with your target column name\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c0f6d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     19\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     20\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785bfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2df1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864e847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be219a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e05a87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['category'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m numeric_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m X \u001b[38;5;241m=\u001b[39m numeric_columns\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Replace \"category\" with your target column name\u001b[39;00m\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets (adjust the test_size as needed)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5400\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5401\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5402\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5403\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5404\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5405\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5406\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5407\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['category'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Identify numeric columns (exclude non-numeric columns)\n",
    "numeric_columns = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Extract features and labels\n",
    "X = numeric_columns.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3a48d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['category'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m numeric_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m X \u001b[38;5;241m=\u001b[39m numeric_columns\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Replace \"category\" with your target column name\u001b[39;00m\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets (adjust the test_size as needed)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5400\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5401\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5402\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5403\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5404\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5405\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5406\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5407\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['category'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Identify numeric columns (exclude non-numeric columns)\n",
    "numeric_columns = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Extract features and labels\n",
    "X = numeric_columns.drop(columns=[\"category\"])  # Replace \"category\" with your target column name\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1f44a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     20\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 21\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     22\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     23\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")  # Replace \"your_dataset.csv\" with the actual file path\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"path\", \"category\"])  # Remove \"path\" and \"category\" columns\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets (adjust the test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1a5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"path\", \"category\"])\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dbcd4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     19\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 20\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# Assuming classification task\u001b[39;00m\n\u001b[0;32m     21\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     22\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"path\", \"category\"])\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Assuming classification task\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters and instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7110e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.9466092983881633\n",
      "Epoch 2/10, Loss: 1.6877502799034119\n",
      "Epoch 3/10, Loss: 1.4259681900342305\n",
      "Epoch 4/10, Loss: 1.1753667891025543\n",
      "Epoch 5/10, Loss: 1.0324740707874298\n",
      "Epoch 6/10, Loss: 0.9578228890895844\n",
      "Epoch 7/10, Loss: 0.9297818541526794\n",
      "Epoch 8/10, Loss: 0.9019160668055216\n",
      "Epoch 9/10, Loss: 0.8747085134188334\n",
      "Epoch 10/10, Loss: 0.8691643873850504\n",
      "Test Accuracy: 63.74%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=[\"path\", \"category\"])\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoader for training and testing datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters and instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41fc3901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.0715126991271973\n",
      "Epoch 2/10, Loss: 1.9674338499704997\n",
      "Epoch 3/10, Loss: 1.8702132105827332\n",
      "Epoch 4/10, Loss: 1.7785246968269348\n",
      "Epoch 5/10, Loss: 1.6897995074590046\n",
      "Epoch 6/10, Loss: 1.6070225437482197\n",
      "Epoch 7/10, Loss: 1.5281978845596313\n",
      "Epoch 8/10, Loss: 1.4552658994992573\n",
      "Epoch 9/10, Loss: 1.3896570404370625\n",
      "Epoch 10/10, Loss: 1.3245909810066223\n",
      "Test Accuracy: 60.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoader for training and testing datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters and instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd5be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[0;32m     35\u001b[0m class_names \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m---> 36\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(y_test, y_pred, target_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2557\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2558\u001b[0m             )\n\u001b[0;32m   2559\u001b[0m         )\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a5dc48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         7\n",
      "           C       0.00      0.00      0.00         0\n",
      "           D       0.50      0.40      0.44         5\n",
      "           F       0.00      0.00      0.00         1\n",
      "           H       0.70      0.95      0.81        55\n",
      "           N       0.00      0.00      0.00         6\n",
      "          Sa       0.67      0.25      0.36        16\n",
      "          Su       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.64      0.64      0.64        91\n",
      "   macro avg       0.23      0.20      0.20        91\n",
      "weighted avg       0.57      0.64      0.58        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get unique class labels\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Generate a classification report with specified labels\n",
    "report = classification_report(y_test, y_pred, labels=range(len(class_names)), target_names=class_names)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34954c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         7\n",
      "           C       0.00      0.00      0.00         0\n",
      "           D       0.50      0.40      0.44         5\n",
      "           F       0.00      0.00      0.00         1\n",
      "           H       0.70      0.95      0.81        55\n",
      "           N       0.00      0.00      0.00         6\n",
      "          Sa       0.67      0.25      0.36        16\n",
      "          Su       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.64      0.64      0.64        91\n",
      "   macro avg       0.23      0.20      0.20        91\n",
      "weighted avg       0.57      0.64      0.58        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get unique class labels\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Generate a classification report with specified labels\n",
    "report = classification_report(y_test, y_pred, labels=range(len(class_names)), target_names=class_names, zero_division=0)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d66a1b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train the classifier on the training data\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    238\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    239\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    240\u001b[0m )\n\u001b[0;32m    241\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    243\u001b[0m     X, y, validate_separately\u001b[38;5;241m=\u001b[39m(check_X_params, check_y_params)\n\u001b[0;32m    244\u001b[0m )\n\u001b[0;32m    246\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:619\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    618\u001b[0m         check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n\u001b[1;32m--> 619\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Map the categories to your specific labels\n",
    "label_mapping = {\n",
    "    \"H\": 0,\n",
    "    \"N\": 1,\n",
    "    \"Sa\": 2,\n",
    "    \"A\": 3,\n",
    "    \"D\": 4,\n",
    "    \"Su\": 5,\n",
    "    \"F\": 6,\n",
    "}\n",
    "\n",
    "# Map the labels to the new category labels\n",
    "y_encoded = y.map(label_mapping)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get the new class labels\n",
    "class_names = list(label_mapping.keys())\n",
    "\n",
    "# Generate a classification report with the specified labels\n",
    "report = classification_report(y_test, y_pred, target_names=class_names, zero_division=0)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39526e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Train the classifier on the training data\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[0;32m     42\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    238\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    239\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    240\u001b[0m )\n\u001b[0;32m    241\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    243\u001b[0m     X, y, validate_separately\u001b[38;5;241m=\u001b[39m(check_X_params, check_y_params)\n\u001b[0;32m    244\u001b[0m )\n\u001b[0;32m    246\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:619\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    618\u001b[0m         check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n\u001b[1;32m--> 619\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Map the categories to your specific labels\n",
    "label_mapping = {\n",
    "    \"H\": 0,\n",
    "    \"N\": 1,\n",
    "    \"Sa\": 2,\n",
    "    \"A\": 3,\n",
    "    \"D\": 4,\n",
    "    \"Su\": 5,\n",
    "    \"F\": 6,\n",
    "}\n",
    "\n",
    "# Map the labels to the new category labels\n",
    "y_encoded = y.map(label_mapping)\n",
    "\n",
    "# Remove rows with missing target values\n",
    "data_cleaned = data.dropna(subset=[\"category\"])\n",
    "X = data_cleaned[[\"valence\", \"arousal\"]].values\n",
    "y_encoded = data_cleaned[\"category\"].map(label_mapping)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get the new class labels\n",
    "class_names = list(label_mapping.keys())\n",
    "\n",
    "# Generate a classification report with the specified labels\n",
    "report = classification_report(y_test, y_pred, target_names=class_names, zero_division=0)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eafefc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in y_encoded: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for missing values in y_encoded\n",
    "missing_values = np.isnan(y_encoded)\n",
    "\n",
    "# Count the number of missing values\n",
    "num_missing_values = np.sum(missing_values)\n",
    "\n",
    "# Print the number of missing values\n",
    "print(f\"Number of missing values in y_encoded: {num_missing_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d14fe080",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load your dataset from the CSV file\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extract features (X) and target (y)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m4\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Assuming your features start from column index 4\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'N', 'Sa', 'A', 'D', 'Su', 'F']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b98ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.14      0.14      0.14         7\n",
      "           N       0.12      0.20      0.15         5\n",
      "          Sa       0.00      0.00      0.00         1\n",
      "           A       0.75      0.71      0.73        55\n",
      "           D       0.33      0.33      0.33         6\n",
      "          Su       0.30      0.19      0.23        16\n",
      "           F       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.52        91\n",
      "   macro avg       0.26      0.37      0.27        91\n",
      "weighted avg       0.55      0.52      0.53        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'N', 'Sa', 'A', 'D', 'Su', 'F']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446a832a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m y_pred_cleaned \u001b[38;5;241m=\u001b[39m clf_cleaned\u001b[38;5;241m.\u001b[39mpredict(X_test_cleaned)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m report_cleaned \u001b[38;5;241m=\u001b[39m classification_report(y_test_cleaned, y_pred_cleaned, target_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_cleaned)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2557\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2558\u001b[0m             )\n\u001b[0;32m   2559\u001b[0m         )\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'N', 'Sa', 'A', 'D', 'Su', 'F', 'C']  # Add 'C' to the class names\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "596b95d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m y_pred_cleaned \u001b[38;5;241m=\u001b[39m clf_cleaned\u001b[38;5;241m.\u001b[39mpredict(X_test_cleaned)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m report_cleaned \u001b[38;5;241m=\u001b[39m classification_report(y_test_cleaned, y_pred_cleaned, target_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_cleaned)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2557\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2558\u001b[0m             )\n\u001b[0;32m   2559\u001b[0m         )\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'N', 'Sa', 'A', 'D', 'Su', 'F', 'C']  # Add 'C' to the class names\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "536ecbee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m y_pred_cleaned \u001b[38;5;241m=\u001b[39m clf_cleaned\u001b[38;5;241m.\u001b[39mpredict(X_test_cleaned)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m report_cleaned \u001b[38;5;241m=\u001b[39m classification_report(y_test_cleaned, y_pred_cleaned, target_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_cleaned)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2557\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2558\u001b[0m             )\n\u001b[0;32m   2559\u001b[0m         )\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'N', 'Sa', 'A', 'D', 'Su', 'F', 'C']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecc03df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.14      0.14      0.14         7\n",
      "           C       0.12      0.20      0.15         5\n",
      "          Sa       0.00      0.00      0.00         1\n",
      "           A       0.75      0.71      0.73        55\n",
      "           D       0.33      0.33      0.33         6\n",
      "          Su       0.30      0.19      0.23        16\n",
      "           F       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.52        91\n",
      "   macro avg       0.26      0.37      0.27        91\n",
      "weighted avg       0.55      0.52      0.53        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77b1f1b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m y_pred_cleaned \u001b[38;5;241m=\u001b[39m clf_cleaned\u001b[38;5;241m.\u001b[39mpredict(X_test_cleaned)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m report_cleaned \u001b[38;5;241m=\u001b[39m classification_report(y_test_cleaned, y_pred_cleaned, target_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_cleaned)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2557\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2558\u001b[0m             )\n\u001b[0;32m   2559\u001b[0m         )\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "182851ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m y_pred_cleaned \u001b[38;5;241m=\u001b[39m clf_cleaned\u001b[38;5;241m.\u001b[39mpredict(X_test_cleaned)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m report_cleaned \u001b[38;5;241m=\u001b[39m classification_report(y_test_cleaned, y_pred_cleaned, target_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_cleaned)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2557\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2558\u001b[0m             )\n\u001b[0;32m   2559\u001b[0m         )\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided, including \"C\"\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1fd9faf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3596495257.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    X_cleaned = np delete(X, missing_indices, axis=0)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided, including \"C\"\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e9baaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.14      0.14      0.14         7\n",
      "           C       0.00      0.00      0.00         0\n",
      "          Sa       0.12      0.20      0.15         5\n",
      "           A       0.00      0.00      0.00         1\n",
      "           D       0.75      0.71      0.73        55\n",
      "          Su       0.33      0.33      0.33         6\n",
      "           F       0.30      0.19      0.23        16\n",
      "           N       0.20      1.00      0.33         1\n",
      "\n",
      "   micro avg       0.52      0.52      0.52        91\n",
      "   macro avg       0.23      0.32      0.24        91\n",
      "weighted avg       0.55      0.52      0.53        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kamil\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided, including \"C\"\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report with labels parameter specified\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned, labels=np.arange(len(class_names)), target_names=class_names)\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e027267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.14      0.14      0.14         7\n",
      "           C       1.00      1.00      1.00         0\n",
      "          Sa       0.12      0.20      0.15         5\n",
      "           A       0.00      0.00      1.00         1\n",
      "           D       0.75      0.71      0.73        55\n",
      "          Su       0.33      0.33      0.33         6\n",
      "           F       0.30      0.19      0.23        16\n",
      "           N       0.20      1.00      0.33         1\n",
      "\n",
      "   micro avg       0.52      0.52      0.52        91\n",
      "   macro avg       0.36      0.45      0.49        91\n",
      "weighted avg       0.55      0.52      0.54        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(\n",
    "    y_test_cleaned, y_pred_cleaned, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1\n",
    ")\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f524f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.9081172943115234\n",
      "Epoch 2/20, Loss: 1.8243767221768696\n",
      "Epoch 3/20, Loss: 1.7449952165285747\n",
      "Epoch 4/20, Loss: 1.6640053391456604\n",
      "Epoch 5/20, Loss: 1.5875739057858784\n",
      "Epoch 6/20, Loss: 1.5193373759587605\n",
      "Epoch 7/20, Loss: 1.4574654499689739\n",
      "Epoch 8/20, Loss: 1.402555247147878\n",
      "Epoch 9/20, Loss: 1.3250059286753337\n",
      "Epoch 10/20, Loss: 1.265359143416087\n",
      "Epoch 11/20, Loss: 1.226907749970754\n",
      "Epoch 12/20, Loss: 1.1856194734573364\n",
      "Epoch 13/20, Loss: 1.1446576317151387\n",
      "Epoch 14/20, Loss: 1.1296043992042542\n",
      "Epoch 15/20, Loss: 1.0955491463343303\n",
      "Epoch 16/20, Loss: 1.0798243880271912\n",
      "Epoch 17/20, Loss: 1.0493240455786388\n",
      "Epoch 18/20, Loss: 1.0300952990849812\n",
      "Epoch 19/20, Loss: 1.0185965696970622\n",
      "Epoch 20/20, Loss: 1.0196878015995026\n",
      "Test Accuracy: 60.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoader for training and testing datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters and instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4932d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.9860150416692097\n",
      "Epoch 2/10, Loss: 1.9025273323059082\n",
      "Epoch 3/10, Loss: 1.8198372721672058\n",
      "Epoch 4/10, Loss: 1.7422237992286682\n",
      "Epoch 5/10, Loss: 1.669829249382019\n",
      "Epoch 6/10, Loss: 1.5959459344546\n",
      "Epoch 7/10, Loss: 1.5330770015716553\n",
      "Epoch 8/10, Loss: 1.4705901543299358\n",
      "Epoch 9/10, Loss: 1.4077855348587036\n",
      "Epoch 10/10, Loss: 1.3521782755851746\n",
      "Test Accuracy: 60.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"] # category as label\n",
    "\n",
    "# Encode the \"category\" column , converting into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets for the evaluating the models performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors (important for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoader for training and testing datasets. Data loaders help efficiently load and iterate through batches of data during training and evaluation.\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "# It consists of two fully connected (linear) layers with a ReLU activation function in between.\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "# specify input dimension, hidden layer dimension, output dimension, learning rate, and the number of training epochs.\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# and instantiate the model\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function (cross entropy loss) and (Adam) optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4538639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.155304789543152\n",
      "Epoch 2/10, Loss: 2.0728355646133423\n",
      "Epoch 3/10, Loss: 1.9876689910888672\n",
      "Epoch 4/10, Loss: 1.9077953894933064\n",
      "Epoch 5/10, Loss: 1.8290136257807414\n",
      "Epoch 6/10, Loss: 1.7580415606498718\n",
      "Epoch 7/10, Loss: 1.6890752514203389\n",
      "Epoch 8/10, Loss: 1.6148264010747273\n",
      "Epoch 9/10, Loss: 1.5453571478525798\n",
      "Epoch 10/10, Loss: 1.480533281962077\n",
      "Test Accuracy: 60.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels, including valence and arousal\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Encode the \"category\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoader for training and testing datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters and instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ad4b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.0847108364105225\n",
      "Epoch 2/10, Loss: 1.972531000773112\n",
      "Epoch 3/10, Loss: 1.865556498368581\n",
      "Epoch 4/10, Loss: 1.7632246414820354\n",
      "Epoch 5/10, Loss: 1.664811134338379\n",
      "Epoch 6/10, Loss: 1.577544093132019\n",
      "Epoch 7/10, Loss: 1.4934890866279602\n",
      "Epoch 8/10, Loss: 1.4219044049580891\n",
      "Epoch 9/10, Loss: 1.346752365430196\n",
      "Epoch 10/10, Loss: 1.272808571656545\n",
      "Test Accuracy: 60.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "y = data[\"category\"] # category as label\n",
    "\n",
    "# Encode the \"category\" column , converting into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets for the evaluating the models performance\n",
    "#     \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors (important for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoader for training and testing datasets. Data loaders help efficiently load and iterate through batches of data during training and evaluation.\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "# It consists of two fully connected (linear) layers with a ReLU activation function in between.\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "# specify input dimension, hidden layer dimension, output dimension, learning rate, and the number of training epochs.\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# and instantiate the model\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function (cross entropy loss) and (Adam) optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "#    accuracy (skilearn)\n",
    "#      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc689ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.14      0.14      0.14         7\n",
      "           C       1.00      1.00      1.00         0\n",
      "          Sa       0.12      0.20      0.15         5\n",
      "           A       0.00      0.00      1.00         1\n",
      "           D       0.75      0.71      0.73        55\n",
      "          Su       0.33      0.33      0.33         6\n",
      "           F       0.30      0.19      0.23        16\n",
      "           N       0.20      1.00      0.33         1\n",
      "\n",
      "   micro avg       0.52      0.52      0.52        91\n",
      "   macro avg       0.36      0.45      0.49        91\n",
      "weighted avg       0.55      0.52      0.54        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset from the CSV file\n",
    "data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = data.iloc[:, 4:].values  # features start from column index 4\n",
    "y = data[\"category\"].values\n",
    "\n",
    "# Encode the target labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Find the indices of missing values in y_encoded\n",
    "missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "# Remove rows with missing values from X and y_encoded\n",
    "X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the cleaned data\n",
    "#  ,  ,   sv ,     \n",
    "clf_cleaned = DecisionTreeClassifier(random_state=42)\n",
    "clf_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the cleaned test data\n",
    "y_pred_cleaned = clf_cleaned.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report\n",
    "report_cleaned = classification_report(\n",
    "    y_test_cleaned, y_pred_cleaned, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1\n",
    ")\n",
    "print(report_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf1f217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6268059331543592\n",
      "Epoch 2/10, Loss: 1.2533464212806857\n",
      "Epoch 3/10, Loss: 1.154113489754346\n",
      "Epoch 4/10, Loss: 1.1270920415313876\n",
      "Epoch 5/10, Loss: 1.103167595911999\n",
      "Epoch 6/10, Loss: 1.1003410585072575\n",
      "Epoch 7/10, Loss: 1.1037178136864487\n",
      "Epoch 8/10, Loss: 1.0948918632098608\n",
      "Epoch 9/10, Loss: 1.0895064959720688\n",
      "Epoch 10/10, Loss: 1.0925423101503022\n",
      "Development Accuracy: 72.25%\n",
      "Test Accuracy: 74.85%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a function to load and preprocess a dataset\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "    y = data[\"category\"]  # Category as labels\n",
    "\n",
    "    # Encode the \"category\" column\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.int64)\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Load and preprocess each dataset\n",
    "X_train_tensor, y_train_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_train.csv\")\n",
    "X_dev_tensor, y_dev_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_dev.csv\")\n",
    "X_test_tensor, y_test_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y_train_tensor.numpy()))  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation on the development dataset\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dev_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_dev = 100 * correct / total\n",
    "print(f\"Development Accuracy: {accuracy_dev:.2f}%\")\n",
    "\n",
    "# Evaluation on the test dataset\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_test = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac6dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.9400830147217731, Dev Accuracy: 69.82%\n",
      "Epoch 2/100, Loss: 1.4755657765330101, Dev Accuracy: 69.82%\n",
      "Epoch 3/100, Loss: 1.2281174854356416, Dev Accuracy: 69.82%\n",
      "Epoch 4/100, Loss: 1.1479236593051834, Dev Accuracy: 70.26%\n",
      "Epoch 5/100, Loss: 1.1225136530642608, Dev Accuracy: 70.93%\n",
      "Epoch 6/100, Loss: 1.1043504500875667, Dev Accuracy: 71.37%\n",
      "Epoch 7/100, Loss: 1.1013535115183617, Dev Accuracy: 71.37%\n",
      "Epoch 8/100, Loss: 1.0966971869371376, Dev Accuracy: 71.15%\n",
      "Epoch 9/100, Loss: 1.0907173254051987, Dev Accuracy: 71.15%\n",
      "Epoch 10/100, Loss: 1.0958102272481334, Dev Accuracy: 72.25%\n",
      "Epoch 11/100, Loss: 1.0953799413174998, Dev Accuracy: 72.25%\n",
      "Epoch 12/100, Loss: 1.0871419042957073, Dev Accuracy: 72.25%\n",
      "Epoch 13/100, Loss: 1.0892922318711573, Dev Accuracy: 72.25%\n",
      "Epoch 14/100, Loss: 1.095302236323454, Dev Accuracy: 72.25%\n",
      "Epoch 15/100, Loss: 1.0888800767003273, Dev Accuracy: 72.25%\n",
      "Epoch 16/100, Loss: 1.092045825354907, Dev Accuracy: 72.25%\n",
      "Epoch 17/100, Loss: 1.0909241474404627, Dev Accuracy: 72.25%\n",
      "Epoch 18/100, Loss: 1.0858010017142004, Dev Accuracy: 72.25%\n",
      "Epoch 19/100, Loss: 1.0845552147651205, Dev Accuracy: 72.25%\n",
      "Epoch 20/100, Loss: 1.0893034849848067, Dev Accuracy: 72.25%\n",
      "Early stopping: No improvement in validation accuracy.\n",
      "Test Accuracy: 74.85%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a function to load and preprocess a dataset\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "    y = data[\"category\"]  # Category as labels\n",
    "\n",
    "    # Encode the \"category\" column\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.int64)\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Load and preprocess each dataset\n",
    "X_train_tensor, y_train_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_train.csv\")\n",
    "X_dev_tensor, y_dev_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_dev.csv\")\n",
    "X_test_tensor, y_test_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y_train_tensor.numpy()))  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 100  # Increase the number of epochs\n",
    "\n",
    "# Instantiate the model\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10  # Number of epochs to wait before stopping\n",
    "best_dev_accuracy = 0.0\n",
    "no_improvement_count = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluation on the development dataset\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy_dev = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}, Dev Accuracy: {accuracy_dev:.2f}%\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if accuracy_dev > best_dev_accuracy:\n",
    "        best_dev_accuracy = accuracy_dev\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    if no_improvement_count >= early_stopping_patience:\n",
    "        print(\"Early stopping: No improvement in validation accuracy.\")\n",
    "        break\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_test = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab94379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.8500124945932506, Dev Accuracy: 69.82%\n",
      "Epoch 2/100, Loss: 1.3438140470154432, Dev Accuracy: 69.82%\n",
      "Epoch 3/100, Loss: 1.1871216066029606, Dev Accuracy: 69.82%\n",
      "Epoch 4/100, Loss: 1.1489367618852733, Dev Accuracy: 70.26%\n",
      "Epoch 5/100, Loss: 1.1265167265522236, Dev Accuracy: 70.48%\n",
      "Epoch 6/100, Loss: 1.108749383566331, Dev Accuracy: 70.70%\n",
      "Epoch 7/100, Loss: 1.1056185705321175, Dev Accuracy: 71.15%\n",
      "Epoch 8/100, Loss: 1.1044289299419947, Dev Accuracy: 71.37%\n",
      "Epoch 9/100, Loss: 1.1073367133432506, Dev Accuracy: 71.15%\n",
      "Epoch 10/100, Loss: 1.097041194536248, Dev Accuracy: 72.25%\n",
      "Epoch 11/100, Loss: 1.0972963766175874, Dev Accuracy: 72.25%\n",
      "Epoch 12/100, Loss: 1.0902388862201147, Dev Accuracy: 72.25%\n",
      "Epoch 13/100, Loss: 1.0898247665288496, Dev Accuracy: 72.25%\n",
      "Epoch 14/100, Loss: 1.0980484473462007, Dev Accuracy: 72.25%\n",
      "Epoch 15/100, Loss: 1.091805792584711, Dev Accuracy: 72.25%\n",
      "Epoch 16/100, Loss: 1.0919670718056815, Dev Accuracy: 72.25%\n",
      "Epoch 17/100, Loss: 1.0932777402352314, Dev Accuracy: 72.25%\n",
      "Epoch 18/100, Loss: 1.08925810517097, Dev Accuracy: 72.25%\n",
      "Epoch 19/100, Loss: 1.090105229494523, Dev Accuracy: 72.25%\n",
      "Epoch 20/100, Loss: 1.0905110544087935, Dev Accuracy: 72.25%\n",
      "Early stopping: No improvement in validation accuracy.\n",
      "Test Accuracy: 74.33%\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.28      0.24      0.26        46\n",
      "           C       1.00      0.00      0.00        18\n",
      "          Sa       1.00      0.00      0.00        13\n",
      "           A       1.00      0.00      0.00        23\n",
      "           D       0.80      0.97      0.88       718\n",
      "          Su       1.00      0.00      0.00        39\n",
      "           F       0.24      0.16      0.19        89\n",
      "           N       1.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.74       974\n",
      "   macro avg       0.79      0.17      0.17       974\n",
      "weighted avg       0.75      0.74      0.68       974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a function to load and preprocess a dataset\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as features\n",
    "    y = data[\"category\"]  # Category as labels\n",
    "\n",
    "    # Encode the \"category\" column\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.int64)\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Load and preprocess each dataset\n",
    "X_train_tensor, y_train_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_train.csv\")\n",
    "X_dev_tensor, y_dev_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_dev.csv\")\n",
    "X_test_tensor, y_test_tensor = load_and_preprocess_dataset(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define your fully connected neural network\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y_train_tensor.numpy()))  # Number of unique classes\n",
    "learning_rate = 0.001\n",
    "epochs = 100  # Increase the number of epochs\n",
    "\n",
    "# Instantiate the model\n",
    "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10  # Number of epochs to wait before stopping\n",
    "best_dev_accuracy = 0.0\n",
    "no_improvement_count = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluation on the development dataset\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy_dev = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}, Dev Accuracy: {accuracy_dev:.2f}%\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if accuracy_dev > best_dev_accuracy:\n",
    "        best_dev_accuracy = accuracy_dev\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    if no_improvement_count >= early_stopping_patience:\n",
    "        print(\"Early stopping: No improvement in validation accuracy.\")\n",
    "        break\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy_test = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}%\")\n",
    "\n",
    "# Calculate precision, recall, and f1-score\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "report_test = classification_report(all_labels, all_preds, target_names=class_names, zero_division=1)\n",
    "print(\"Classification Report on Test Data:\")\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7577a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for emotic_radiant_fog_160_dev.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       1.00      1.00      1.00        33\n",
      "           C       1.00      1.00      1.00         6\n",
      "          Sa       1.00      1.00      1.00        13\n",
      "           A       1.00      1.00      1.00         6\n",
      "           D       1.00      1.00      1.00       317\n",
      "          Su       1.00      1.00      1.00        16\n",
      "           F       1.00      1.00      1.00        50\n",
      "           N       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00       454\n",
      "   macro avg       1.00      1.00      1.00       454\n",
      "weighted avg       1.00      1.00      1.00       454\n",
      "\n",
      "Classification Report for emotic_radiant_fog_160_train.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       1.00      1.00      1.00       340\n",
      "           C       1.00      1.00      1.00        51\n",
      "          Sa       1.00      1.00      1.00        58\n",
      "           A       1.00      1.00      1.00        31\n",
      "           D       1.00      1.00      1.00      1985\n",
      "          Su       1.00      1.00      1.00       211\n",
      "           F       1.00      1.00      1.00       339\n",
      "           N       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           1.00      3092\n",
      "   macro avg       1.00      1.00      1.00      3092\n",
      "weighted avg       1.00      1.00      1.00      3092\n",
      "\n",
      "Classification Report for emotic_radiant_fog_160_test.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       1.00      1.00      1.00        46\n",
      "           C       1.00      1.00      1.00        18\n",
      "          Sa       1.00      1.00      1.00        13\n",
      "           A       1.00      1.00      1.00        23\n",
      "           D       1.00      1.00      1.00       718\n",
      "          Su       1.00      1.00      1.00        39\n",
      "           F       1.00      1.00      1.00        89\n",
      "           N       1.00      1.00      1.00        28\n",
      "\n",
      "    accuracy                           1.00       974\n",
      "   macro avg       1.00      1.00      1.00       974\n",
      "weighted avg       1.00      1.00      1.00       974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# List of dataset file names\n",
    "dataset_files = [\"emotic_radiant_fog_160_dev.csv\", \"emotic_radiant_fog_160_train.csv\", \"emotic_radiant_fog_160_test.csv\"]\n",
    "\n",
    "for dataset_file in dataset_files:\n",
    "    # Load the dataset from the CSV file\n",
    "    data = pd.read_csv(dataset_file)\n",
    "\n",
    "    # Extract features (X) and target (y)\n",
    "    X = data.iloc[:, 4:].values  # Features start from column index 4\n",
    "    y = data[\"category\"].values\n",
    "\n",
    "    # Encode the target labels to numeric values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Create a new decision tree classifier and train it on the dataset\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X, y_encoded)\n",
    "\n",
    "    # Predict on the dataset\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # Generate a classification report\n",
    "    report = classification_report(\n",
    "        y_encoded, y_pred, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1\n",
    "    )\n",
    "    \n",
    "    # Print the classification report\n",
    "    print(f\"Classification Report for {dataset_file}:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f474e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (decision tree) for Combined Datasets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       1.00      1.00      1.00       419\n",
      "           C       1.00      1.00      1.00        75\n",
      "          Sa       1.00      1.00      1.00        84\n",
      "           A       1.00      1.00      1.00        60\n",
      "           D       1.00      1.00      1.00      3020\n",
      "          Su       1.00      1.00      1.00       266\n",
      "           F       1.00      1.00      1.00       478\n",
      "           N       1.00      1.00      1.00       118\n",
      "\n",
      "    accuracy                           1.00      4520\n",
      "   macro avg       1.00      1.00      1.00      4520\n",
      "weighted avg       1.00      1.00      1.00      4520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# List of dataset file names\n",
    "dataset_files = [\"emotic_radiant_fog_160_dev.csv\", \"emotic_radiant_fog_160_train.csv\", \"emotic_radiant_fog_160_test.csv\"]\n",
    "\n",
    "# Initialize empty lists to store data from all datasets\n",
    "X_combined = []\n",
    "y_combined = []\n",
    "\n",
    "for dataset_file in dataset_files:\n",
    "    # Load the dataset from the CSV file\n",
    "    data = pd.read_csv(dataset_file)\n",
    "\n",
    "    # Extract features (X) and target (y)\n",
    "    X = data.iloc[:, 4:].values  # Features start from column index 4\n",
    "    y = data[\"category\"].values\n",
    "\n",
    "    # Encode the target labels to numeric values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Append data to the combined lists\n",
    "    X_combined.append(X)\n",
    "    y_combined.append(y_encoded)\n",
    "\n",
    "# Concatenate data from all datasets\n",
    "X_combined = np.concatenate(X_combined, axis=0)\n",
    "y_combined = np.concatenate(y_combined, axis=0)\n",
    "\n",
    "# Create a new decision tree classifier and train it on the combined data\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict on the combined dataset\n",
    "y_pred_combined = clf.predict(X_combined)\n",
    "\n",
    "# Generate a classification report for the combined dataset\n",
    "report_combined = classification_report(\n",
    "    y_combined, y_pred_combined, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1\n",
    ")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report (decision tree) for Combined Datasets:\")\n",
    "print(report_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3544ff54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Create a new Naive Bayes classifier and train it on the training data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m nb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m---> 38\u001b[0m nb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train_cleaned, y_train_cleaned)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Predict on the dev data\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y_dev_pred_cleaned \u001b[38;5;241m=\u001b[39m nb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_dev_cleaned)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:772\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    770\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count(X, Y)\n\u001b[0;32m    773\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:894\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    893\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 894\u001b[0m     check_non_negative(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultinomialNB (input X)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1490\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_and_clean_data(file_path):\n",
    "    # Load your dataset from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract features (X) and target (y)\n",
    "    X = data.iloc[:, 4:].values  # features start from column index 4\n",
    "    y = data[\"category\"].values\n",
    "\n",
    "    # Encode the target labels to numeric values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Find the indices of missing values in y_encoded\n",
    "    missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "    # Remove rows with missing values from X and y_encoded\n",
    "    X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "    y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "    return X_cleaned, y_cleaned\n",
    "\n",
    "# Define your class names based on the categories you provided\n",
    "class_names = ['H', 'C', 'Sa', 'A', 'D', 'Su', 'F', 'N']\n",
    "\n",
    "# Load and clean each dataset\n",
    "X_train_cleaned, y_train_cleaned = load_and_clean_data(\"emotic_radiant_fog_160_train.csv\")\n",
    "X_dev_cleaned, y_dev_cleaned = load_and_clean_data(\"emotic_radiant_fog_160_dev.csv\")\n",
    "X_test_cleaned, y_test_cleaned = load_and_clean_data(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Create a new Naive Bayes classifier and train it on the training data\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the dev data\n",
    "y_dev_pred_cleaned = nb_classifier.predict(X_dev_cleaned)\n",
    "\n",
    "# Generate a classification report for dev data\n",
    "report_dev_cleaned = classification_report(\n",
    "    y_dev_cleaned, y_dev_pred_cleaned, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1\n",
    ")\n",
    "print(\"Classification Report for Dev Dataset:\")\n",
    "print(report_dev_cleaned)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred_cleaned = nb_classifier.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report for test data\n",
    "report_test_cleaned = classification_report(\n",
    "    y_test_cleaned, y_test_pred_cleaned, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1\n",
    ")\n",
    "print(\"Classification Report for Test Dataset:\")\n",
    "print(report_test_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b40fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dev Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.30      0.21      0.25        33\n",
      "           C       0.00      0.00      1.00         6\n",
      "           D       0.09      0.15      0.11        13\n",
      "           F       0.08      0.33      0.13         6\n",
      "           H       0.96      0.47      0.63       317\n",
      "           N       0.05      0.50      0.09        16\n",
      "          Sa       0.39      0.22      0.28        50\n",
      "          Su       0.03      0.08      0.05        13\n",
      "\n",
      "    accuracy                           0.39       454\n",
      "   macro avg       0.24      0.25      0.32       454\n",
      "weighted avg       0.74      0.39      0.51       454\n",
      "\n",
      "\n",
      "Classification Report for Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.34      0.24      0.28        46\n",
      "           C       0.17      0.06      0.08        18\n",
      "           D       0.03      0.08      0.04        13\n",
      "           F       0.21      0.30      0.25        23\n",
      "           H       0.93      0.42      0.58       718\n",
      "           N       0.06      0.67      0.12        39\n",
      "          Sa       0.32      0.21      0.26        89\n",
      "          Su       0.11      0.29      0.16        28\n",
      "\n",
      "    accuracy                           0.39       974\n",
      "   macro avg       0.27      0.28      0.22       974\n",
      "weighted avg       0.74      0.39      0.48       974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define a function to load and preprocess a dataset\n",
    "def load_and_preprocess_dataset(file_path):\n",
    "    # Load the dataset from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract features (X) and target (y)\n",
    "    X = data.iloc[:, 4:].values  # Assuming your features start from column index 4\n",
    "    y = data[\"category\"].values\n",
    "\n",
    "    # Encode the target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Find the indices of missing values in y_encoded\n",
    "    missing_indices = np.where(y_encoded == -1)[0]\n",
    "\n",
    "    # Remove rows with missing values from X and y_encoded\n",
    "    X_cleaned = np.delete(X, missing_indices, axis=0)\n",
    "    y_cleaned = np.delete(y_encoded, missing_indices)\n",
    "\n",
    "    return X_cleaned, y_cleaned\n",
    "\n",
    "# Load and preprocess all three datasets\n",
    "X_train_cleaned, y_train_cleaned = load_and_preprocess_dataset(\"emotic_radiant_fog_160_train.csv\")\n",
    "X_dev_cleaned, y_dev_cleaned = load_and_preprocess_dataset(\"emotic_radiant_fog_160_dev.csv\")\n",
    "X_test_cleaned, y_test_cleaned = load_and_preprocess_dataset(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Create a new Gaussian Naive Bayes classifier and train it on the training data\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on the dev data\n",
    "y_dev_pred_cleaned = gnb_classifier.predict(X_dev_cleaned)\n",
    "\n",
    "# Generate a classification report for the dev data\n",
    "report_dev = classification_report(y_dev_cleaned, y_dev_pred_cleaned, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(\"Classification Report for Dev Data:\")\n",
    "print(report_dev)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred_cleaned = gnb_classifier.predict(X_test_cleaned)\n",
    "\n",
    "# Generate a classification report for the test data\n",
    "report_test = classification_report(y_test_cleaned, y_test_pred_cleaned, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(\"\\nClassification Report for Test Data:\")\n",
    "print(report_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5537303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00       419\n",
      "           C       1.00      1.00      1.00        75\n",
      "           D       1.00      1.00      1.00        84\n",
      "           F       1.00      1.00      1.00        60\n",
      "           H       1.00      1.00      1.00      3020\n",
      "           N       1.00      1.00      1.00       266\n",
      "          Sa       1.00      1.00      1.00       478\n",
      "          Su       1.00      1.00      1.00       118\n",
      "\n",
      "    accuracy                           1.00      4520\n",
      "   macro avg       1.00      1.00      1.00      4520\n",
      "weighted avg       1.00      1.00      1.00      4520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your datasets from the CSV files\n",
    "train_data = pd.read_csv(\"emotic_radiant_fog_160_train.csv\")\n",
    "dev_data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "test_data = pd.read_csv(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Concatenate your datasets into a single dataset\n",
    "combined_data = pd.concat([train_data, dev_data, test_data], ignore_index=True)\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = combined_data.iloc[:, 4:].values  # Features start from column index 4\n",
    "y = combined_data[\"category\"].values\n",
    "\n",
    "# Encode the target labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create a decision tree classifier and train it on the combined data\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X, y_encoded)\n",
    "\n",
    "# Predict on the combined data\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# Generate an overall classification report\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(y_encoded, y_pred, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1)\n",
    "print(\"Overall Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05dca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'kernel': 'linear'}\n",
      "Best Accuracy: 0.7048672566371682\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.61      0.30      0.41       419\n",
      "           C       0.86      0.68      0.76        75\n",
      "           D       0.50      0.10      0.16        84\n",
      "           F       0.71      0.28      0.40        60\n",
      "           H       0.73      0.98      0.84      3020\n",
      "           N       1.00      0.00      0.00       266\n",
      "          Sa       0.50      0.13      0.21       478\n",
      "          Su       1.00      0.00      0.00       118\n",
      "\n",
      "    accuracy                           0.72      4520\n",
      "   macro avg       0.74      0.31      0.35      4520\n",
      "weighted avg       0.71      0.72      0.64      4520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load your datasets from the CSV files\n",
    "train_data = pd.read_csv(\"emotic_radiant_fog_160_train.csv\")\n",
    "dev_data = pd.read_csv(\"emotic_radiant_fog_160_dev.csv\")\n",
    "test_data = pd.read_csv(\"emotic_radiant_fog_160_test.csv\")\n",
    "\n",
    "# Concatenate your datasets into a single dataset\n",
    "combined_data = pd.concat([train_data, dev_data, test_data], ignore_index=True)\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = combined_data.iloc[:, 4:].values  # Features start from column index 4\n",
    "y = combined_data[\"category\"].values\n",
    "\n",
    "# Encode the target labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # Different values of C\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Different kernel functions\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Perform an exhaustive grid search\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the SVM classifier with the best parameters on the combined data\n",
    "best_svc = SVC(C=grid_search.best_params_['C'], kernel=grid_search.best_params_['kernel'])\n",
    "best_svc.fit(X, y_encoded)\n",
    "\n",
    "# Predict on the combined data\n",
    "y_pred = best_svc.predict(X)\n",
    "\n",
    "# Generate an overall classification report\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(y_encoded, y_pred, labels=np.arange(len(class_names)), target_names=class_names, zero_division=1)\n",
    "print(\"Overall Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad566a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

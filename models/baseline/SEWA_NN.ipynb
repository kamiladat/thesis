{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2Wp5/msfTLgFINq4XJrvb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8ZVTBywYl8Q","executionInfo":{"status":"ok","timestamp":1699488144314,"user_tz":-60,"elapsed":14538,"user":{"displayName":"Kamila Datbayev","userId":"02458196223707094393"}},"outputId":"61552e9f-4762-426b-8ec1-f7dbd002d886"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["def predict_on_dev(model, dev_loader):\n","  y_valence_true = []\n","  y_valence_pred = []\n","  y_arousal_true = []\n","  y_arousal_pred = []\n","\n","  model.eval()\n","  with torch.no_grad():\n","      for inputs, labels in dev_loader:\n","          outputs = model(inputs)\n","          labels_valence = labels[:, 0]\n","          labels_arousal = labels[:, 1]\n","          outputs_valence = outputs[:, 0]\n","          outputs_arousal = outputs[:, 1]\n","\n","          y_valence_true.extend(labels_valence.cpu().numpy())\n","          y_valence_pred.extend(outputs_valence.cpu().numpy())\n","          y_arousal_true.extend(labels_arousal.cpu().numpy())\n","          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n","\n","  # Calculate metrics\n","  mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n","  rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n","  mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n","  rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n","\n","  return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)"],"metadata":{"id":"Oodc9v7bYyN8","executionInfo":{"status":"ok","timestamp":1699488250773,"user_tz":-60,"elapsed":252,"user":{"displayName":"Kamila Datbayev","userId":"02458196223707094393"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import torch.nn as nn\n","import numpy as np\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from math import sqrt\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Define a function to load and preprocess a dataset\n","def load_and_preprocess_dataset(filename):\n","    data = pd.read_csv(filename)\n","\n","    # Extract features and labels\n","    X = data.loc[:, 'emb_0':].values\n","    y = data[[\"valence\", \"arousal\"]].values  # Use valence and arousal as labels\n","\n","    # Convert data to PyTorch tensors\n","    X_tensor = torch.tensor(X, dtype=torch.float32)\n","    y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","    return X_tensor, y_tensor\n","\n","# Load and preprocess each dataset\n","X_train_tensor, y_train_tensor = load_and_preprocess_dataset(\"/content/drive/MyDrive/SEWA_radiant_fog_160_train.csv\")\n","X_dev_tensor, y_dev_tensor = load_and_preprocess_dataset(\"/content/drive/MyDrive/SEWA_radiant_fog_160_dev.csv\")\n","X_test_tensor, y_test_tensor = load_and_preprocess_dataset(\"/content/drive/MyDrive/SEWA_radiant_fog_160_test.csv\")\n","\n","# Create DataLoaders for each dataset\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n","dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n","\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Define your fully connected neural network for regression with tanh activation\n","class FullyConnectedNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FullyConnectedNN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","        self.tanh = nn.Tanh()  # Apply tanh activation to the output\n","        self.output_dim = output_dim\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.tanh(x)  # Apply tanh activation to the output\n","        return x\n","\n","# Define hyperparameters\n","input_dim = X_train_tensor.shape[1]\n","hidden_dim = 64\n","output_dim = 2\n","learning_rate = 0.001\n","epochs = 100\n","\n","# Instantiate the model\n","model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n","\n","# Define separate loss functions for arousal and valence\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, yhat, y):\n","        return torch.sqrt(self.mse(yhat, y))\n","\n","criterion_arousal = RMSELoss()\n","criterion_valence = RMSELoss()\n","\n","# Define optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Define early stopping parameters\n","patience = 10  # Number of epochs to wait for improvement\n","min_val_loss = float('inf')\n","counter = 0  # Counter for epochs without improvement\n","best_val_loss = float('inf')\n","best_epoch = 0\n","best_mae_arousal = float('inf')\n","best_mae_valence = float('inf')\n","best_rmse_arousal = float('inf')\n","best_rmse_valence = float('inf')\n","stop_training = False\n","\n","# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","\n","        # Split predicted values into arousal and valence\n","        predicted_arousal = outputs[:, 0]\n","        predicted_valence = outputs[:, 1]\n","\n","        # Split ground truth labels into arousal and valence\n","        labels_arousal = labels[:, 0]\n","        labels_valence = labels[:, 1]\n","\n","        # Calculate separate losses for arousal and valence\n","        loss_arousal = criterion_arousal(predicted_arousal, labels_arousal)\n","        loss_valence = criterion_valence(predicted_valence, labels_valence)\n","        loss = loss_arousal + loss_valence\n","\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    # Predict on dev data using the trained model\n","    model.eval()\n","    dev_mae_valence, dev_rmse_valence, dev_mae_arousal, dev_rmse_arousal = predict_on_dev(model, dev_loader)\n","    general_rmse_metric = (dev_rmse_valence + dev_rmse_arousal) / 2.\n","\n","    print(f\"Validation RMSE: {general_rmse_metric:.4f}\")\n","\n","    # Check Early stopping criteria\n","    if general_rmse_metric < min_val_loss:  # Check if the validation loss has improved\n","      min_val_loss = general_rmse_metric\n","      counter = 0\n","\n","    # Save the model weights if RMSE is lower than the best value\n","      #if dev_rmse_arousal < best_rmse_arousal and dev_rmse_valence < best_rmse_valence:\n","      best_rmse_arousal = dev_rmse_arousal\n","      best_mae_arousal = dev_mae_arousal\n","      best_rmse_valence = dev_rmse_valence\n","      best_mae_valence = dev_mae_valence\n","      best_epoch = epoch\n","      torch.save(model.state_dict(), 'best_model_SEWA.pth')\n","    else:\n","        counter += 1\n","    # If the validation loss hasn't improved for 'patience' epochs, set the stop_training variable\n","    if counter >= patience:\n","        print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n","        stop_training = True\n","    if stop_training:\n","        break\n","\n","# Print the last best results and epoch\n","if stop_training:\n","    print(f\"Best RMSE Arousal: {best_rmse_arousal:.4f} at epoch {best_epoch + 1}\")\n","    print(f\"Best MAE Arousal: {best_mae_arousal:.4f} at epoch {best_epoch + 1}\")\n","    print(f\"Best RMSE Valence: {best_rmse_valence:.4f} at epoch {best_epoch + 1}\")\n","    print(f\"Best MAE Valence: {best_mae_valence:.4f} at epoch {best_epoch + 1}\")\n","else:\n","    print(\"Training completed without early stopping\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QygRxfL_Y_R-","executionInfo":{"status":"ok","timestamp":1699488338511,"user_tz":-60,"elapsed":18282,"user":{"displayName":"Kamila Datbayev","userId":"02458196223707094393"}},"outputId":"a612fc9f-a6a3-43a9-c40b-28ccdd573dc1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation RMSE: 0.0889\n","Validation RMSE: 0.0887\n","Validation RMSE: 0.0917\n","Validation RMSE: 0.0892\n","Validation RMSE: 0.0905\n","Validation RMSE: 0.0897\n","Validation RMSE: 0.0899\n","Validation RMSE: 0.0903\n","Validation RMSE: 0.0910\n","Validation RMSE: 0.0940\n","Validation RMSE: 0.0911\n","Validation RMSE: 0.0932\n","Early stopping at epoch 12 due to no improvement in validation loss.\n","Best RMSE Arousal: 0.0804 at epoch 2\n","Best MAE Arousal: 0.0594 at epoch 2\n","Best RMSE Valence: 0.0971 at epoch 2\n","Best MAE Valence: 0.0685 at epoch 2\n"]}]},{"cell_type":"code","source":["def evaluate_on_test(model, test_loader):\n","    model.eval()\n","    y_valence_true = []\n","    y_valence_pred = []\n","    y_arousal_true = []\n","    y_arousal_pred = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            labels_valence = labels[:, 0]\n","            labels_arousal = labels[:, 1]\n","            outputs_valence = outputs[:, 0]\n","            outputs_arousal = outputs[:, 1]\n","\n","            y_valence_true.extend(labels_valence.cpu().numpy().flatten())\n","            y_valence_pred.extend(outputs_valence.cpu().numpy().flatten())\n","            y_arousal_true.extend(labels_arousal.cpu().numpy().flatten())\n","            y_arousal_pred.extend(outputs_arousal.cpu().numpy().flatten())\n","\n","    # Calculate metrics\n","    mae_valence_test = mean_absolute_error(y_valence_true, y_valence_pred)\n","    rmse_valence_test = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n","    mae_arousal_test = mean_absolute_error(y_arousal_true, y_arousal_pred)\n","    rmse_arousal_test = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n","\n","    return mae_valence_test, rmse_valence_test, mae_arousal_test, rmse_arousal_test"],"metadata":{"id":"K6NC3-6pZn4i","executionInfo":{"status":"ok","timestamp":1699488370564,"user_tz":-60,"elapsed":245,"user":{"displayName":"Kamila Datbayev","userId":"02458196223707094393"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Load the best model\n","model.load_state_dict(torch.load('/content/best_model_SEWA.pth'))\n","model.eval()\n","\n","# Call the evaluate_on_test function\n","mae_valence_test, rmse_valence_test, mae_arousal_test, rmse_arousal_test = evaluate_on_test(model, test_loader)\n","\n","# Print the results for the test dataset\n","print(f\"Test MAE Valence: {mae_valence_test:.4f}, Test RMSE Valence: {rmse_valence_test:.4f}\")\n","print(f\"Test MAE Arousal: {mae_arousal_test:.4f}, Test RMSE Arousal: {rmse_arousal_test:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn-CcL7tZqmp","executionInfo":{"status":"ok","timestamp":1699488388387,"user_tz":-60,"elapsed":237,"user":{"displayName":"Kamila Datbayev","userId":"02458196223707094393"}},"outputId":"7274d841-5dde-45ab-dabf-847f69dfd4ca"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Test MAE Valence: 0.0928, Test RMSE Valence: 0.1178\n","Test MAE Arousal: 0.0878, Test RMSE Arousal: 0.1124\n"]}]},{"cell_type":"code","source":["#Only arousal\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import GridSearchCV, PredefinedSplit\n","from math import sqrt\n","\n","# Load the training dataset\n","train_data = pd.read_csv(\"/content/drive/MyDrive/SEWA_radiant_fog_160_train.csv\")\n","\n","# Load the dev dataset\n","dev_data = pd.read_csv(\"/content/drive/MyDrive/SEWA_radiant_fog_160_dev.csv\")\n","\n","# Load the test dataset\n","test_data = pd.read_csv(\"/content/drive/MyDrive/SEWA_radiant_fog_160_test.csv\")\n","\n","# Extract features (X) and target (y) for arousal in the training data\n","X_train = train_data.iloc[:, 5:].values  # Features start from column index 5\n","y_arousal_train = train_data[\"arousal\"].values\n","\n","# Extract features (X) and target (y) for arousal in the dev data\n","X_dev = dev_data.iloc[:, 5:].values\n","y_arousal_dev = dev_data[\"arousal\"].values\n","\n","# Extract features (X) and target (y) for arousal in the test data\n","X_test = test_data.iloc[:, 5:].values\n","y_arousal_test = test_data[\"arousal\"].values\n","\n","\n","# Create a parameter grid for SVR\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n","}\n","\n","# Concatenate train and dev\n","concat_x_train_dev_arousal = np.concatenate((X_train, X_dev), axis=0)\n","concat_y_arousal_train_dev = np.concatenate((y_arousal_train, y_arousal_dev), axis=0)\n","\n","# Generate indices for training and development parts\n","split_index_arousal = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_dev.shape[0])]\n","\n","# Create PredefinedSplit\n","pds_arousal = PredefinedSplit(test_fold=split_index_arousal)\n","\n","# Use PredefinedSplit in GridSearchCV for Arousal\n","svr_arousal = SVR()\n","grid_search_arousal = GridSearchCV(svr_arousal, param_grid, cv=pds_arousal, scoring='neg_mean_squared_error')\n","grid_search_arousal.fit(concat_x_train_dev_arousal, concat_y_arousal_train_dev)\n","\n","# Get the best estimators\n","best_svr_arousal = grid_search_arousal.best_estimator_\n","\n","# Predict on the dev data for arousal\n","y_arousal_dev_pred = best_svr_arousal.predict(X_dev)\n","\n","# Calculate regression metrics for arousal on the dev data\n","mse_arousal_dev = mean_squared_error(y_arousal_dev, y_arousal_dev_pred)\n","r2_arousal_dev = sqrt(mse_arousal_dev)\n","\n","print(\"Results for Arousal on Dev Data:\")\n","print(\"Arousal - Mean Squared Error:\", mse_arousal_dev)\n","print(\"Arousal - R-squared:\", r2_arousal_dev)\n","\n","# Predict on the test data for arousal\n","y_arousal_test_pred = best_svr_arousal.predict(X_test)\n","\n","# Calculate regression metrics for arousal on the test data\n","mse_arousal_test = mean_squared_error(y_arousal_test, y_arousal_test_pred)\n","r2_arousal_test = sqrt(mse_arousal_test)\n","\n","print(\"Results for Arousal on Test Data:\")\n","print(\"Arousal - Mean Squared Error:\", mse_arousal_test)\n","print(\"Arousal - R-squared:\", r2_arousal_test)"],"metadata":{"id":"gDUeWiJAbA_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Only valence\n","import numpy as np\n","import pandas as pd\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import GridSearchCV, PredefinedSplit\n","from math import sqrt\n","\n","# Load the training dataset\n","train_data = pd.read_csv(\"/content/drive/MyDrive/SEWA_radiant_fog_160_train.csv\")\n","\n","# Load the dev dataset\n","dev_data = pd.read_csv(\"/content/drive/MyDrive/SEWA_radiant_fog_160_dev.csv\")\n","\n","# Load the test dataset\n","test_data = pd.read_csv(\"/content/drive/MyDrive/SEWA_radiant_fog_160_test.csv\")\n","\n","# Extract features (X) and target (y) for valence in the training data\n","X_train = train_data.iloc[:, 5:].values  # Features start from column index 4\n","y_valence_train = train_data[\"valence\"].values\n","\n","# Extract features (X) and target (y) for valence in the dev data\n","X_dev = dev_data.iloc[:, 5:].values\n","y_valence_dev = dev_data[\"valence\"].values\n","\n","# Extract features (X) and target (y) for valence in the test data\n","X_test = test_data.iloc[:, 5:].values\n","y_valence_test = test_data[\"valence\"].values\n","\n","# Create a parameter grid for SVR\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n","}\n","\n","# Concatenate train and dev\n","concat_x_train_dev_valence = np.concatenate((X_train, X_dev), axis=0)\n","concat_y_valence_train_dev = np.concatenate((y_valence_train, y_valence_dev), axis=0)\n","\n","# Generate indices for training and development parts\n","split_index_valence = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_dev.shape[0])]\n","\n","# Create PredefinedSplit\n","pds_valence = PredefinedSplit(test_fold=split_index_valence)\n","\n","# Use PredefinedSplit in GridSearchCV for Valence\n","svr_valence = SVR()\n","grid_search_valence = GridSearchCV(svr_valence, param_grid, cv=pds_valence, scoring='neg_mean_squared_error')\n","grid_search_valence.fit(concat_x_train_dev_valence, concat_y_valence_train_dev)\n","\n","# Get the best estimators\n","best_svr_valence = grid_search_valence.best_estimator_\n","\n","# Predict on the dev data for valence\n","y_valence_dev_pred = best_svr_valence.predict(X_dev)\n","\n","# Calculate regression metrics for valence on the dev data\n","mse_valence_dev = mean_squared_error(y_valence_dev, y_valence_dev_pred)\n","r2_valence_dev = sqrt(mse_valence_dev)\n","\n","print(\"Results for Valence on Dev Data:\")\n","print(\"Valence - Mean Squared Error:\", mse_valence_dev)\n","print(\"Valence - R-squared:\", r2_valence_dev)\n","\n","# Predict on the test data for valence\n","y_valence_test_pred = best_svr_valence.predict(X_test)\n","\n","# Calculate regression metrics for valence on the test data\n","mse_valence_test = mean_squared_error(y_valence_test, y_valence_test_pred)\n","r2_valence_test = sqrt(mse_valence_test)\n","\n","print(\"Results for Valence on Test Data:\")\n","print(\"Valence - Mean Squared Error:\", mse_valence_test)\n","print(\"Valence - R-squared:\", r2_valence_test)"],"metadata":{"id":"U6-UP_svbE-f"},"execution_count":null,"outputs":[]}]}
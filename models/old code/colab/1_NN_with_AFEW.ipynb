{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3tn3l9h3jA7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqJk-KnC9n8I"
      },
      "outputs": [],
      "source": [
        "def predict_on_dev(model, dev_loader):\n",
        "  y_valence_true = []\n",
        "  y_valence_pred = []\n",
        "  y_arousal_true = []\n",
        "  y_arousal_pred = []\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in dev_loader:\n",
        "          outputs = model(inputs)\n",
        "          labels_valence = labels[:, 0]\n",
        "          labels_arousal = labels[:, 1]\n",
        "          outputs_valence = outputs[:, 0]\n",
        "          outputs_arousal = outputs[:, 1]\n",
        "\n",
        "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
        "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
        "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
        "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
        "\n",
        "  # Calculate metrics\n",
        "  mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
        "  rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
        "  mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
        "  rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
        "\n",
        "  return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwZ7siFc_CdW",
        "outputId": "7b4e383b-d46b-4f10-c392-9f218ae656a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 0.1125\n",
            "Validation RMSE: 0.1127\n",
            "Validation RMSE: 0.1114\n",
            "Validation RMSE: 0.1169\n",
            "Validation RMSE: 0.1203\n",
            "Validation RMSE: 0.1201\n",
            "Validation RMSE: 0.1207\n",
            "Validation RMSE: 0.1203\n",
            "Validation RMSE: 0.1270\n",
            "Validation RMSE: 0.1211\n",
            "Validation RMSE: 0.1309\n",
            "Validation RMSE: 0.1293\n",
            "Validation RMSE: 0.1250\n",
            "Early stopping at epoch 13 due to no improvement in validation loss.\n",
            "Best RMSE Arousal: 0.1056 at epoch 3\n",
            "Best MAE Arousal: 0.0810 at epoch 3\n",
            "Best RMSE Valence: 0.1171 at epoch 3\n",
            "Best MAE Valence: 0.0865 at epoch 3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define a function to load and preprocess a dataset\n",
        "def load_and_preprocess_dataset(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Extract features and labels\n",
        "    X = data.loc[:, 'emb_0':].values\n",
        "    y = data[[\"valence\", \"arousal\"]].values/10.  # Use valence and arousal as labels and normalize them\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    return X_tensor, y_tensor\n",
        "\n",
        "# Load and preprocess each dataset\n",
        "X_train_tensor, y_train_tensor = load_and_preprocess_dataset(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv\")\n",
        "X_dev_tensor, y_dev_tensor = load_and_preprocess_dataset(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv\")\n",
        "X_test_tensor, y_test_tensor = load_and_preprocess_dataset(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv\")\n",
        "\n",
        "# Create DataLoaders for each dataset\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define your fully connected neural network for regression with tanh activation\n",
        "class FullyConnectedNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FullyConnectedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()  # Apply tanh activation to the output\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.tanh(x)  # Apply tanh activation to the output\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 2\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Instantiate the model\n",
        "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define separate loss functions for arousal and valence\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, yhat, y):\n",
        "        return torch.sqrt(self.mse(yhat, y))\n",
        "\n",
        "criterion_arousal = RMSELoss()\n",
        "criterion_valence = RMSELoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define early stopping parameters\n",
        "patience = 10  # Number of epochs to wait for improvement\n",
        "min_val_loss = float('inf')\n",
        "counter = 0  # Counter for epochs without improvement\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "best_mae_arousal = float('inf')\n",
        "best_mae_valence = float('inf')\n",
        "best_rmse_arousal = float('inf')\n",
        "best_rmse_valence = float('inf')\n",
        "stop_training = False\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Split predicted values into arousal and valence\n",
        "        predicted_arousal = outputs[:, 0]\n",
        "        predicted_valence = outputs[:, 1]\n",
        "\n",
        "        # Split ground truth labels into arousal and valence\n",
        "        labels_arousal = labels[:, 0]\n",
        "        labels_valence = labels[:, 1]\n",
        "\n",
        "        # Calculate separate losses for arousal and valence\n",
        "        loss_arousal = criterion_arousal(predicted_arousal, labels_arousal)\n",
        "        loss_valence = criterion_valence(predicted_valence, labels_valence)\n",
        "        loss = loss_arousal + loss_valence\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Predict on dev data using the trained model\n",
        "    model.eval()\n",
        "    dev_mae_valence, dev_rmse_valence, dev_mae_arousal, dev_rmse_arousal = predict_on_dev(model, dev_loader)\n",
        "    general_rmse_metric = (dev_rmse_valence + dev_rmse_arousal) / 2.\n",
        "\n",
        "    print(f\"Validation RMSE: {general_rmse_metric:.4f}\")\n",
        "\n",
        "    # Check Early stopping criteria\n",
        "    if general_rmse_metric < min_val_loss:  # Check if the validation loss has improved\n",
        "      min_val_loss = general_rmse_metric\n",
        "      counter = 0\n",
        "\n",
        "    # Save the model weights if RMSE is lower than the best value\n",
        "      #if dev_rmse_arousal < best_rmse_arousal and dev_rmse_valence < best_rmse_valence:\n",
        "      best_rmse_arousal = dev_rmse_arousal\n",
        "      best_mae_arousal = dev_mae_arousal\n",
        "      best_rmse_valence = dev_rmse_valence\n",
        "      best_mae_valence = dev_mae_valence\n",
        "      best_epoch = epoch\n",
        "      torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "    # If the validation loss hasn't improved for 'patience' epochs, set the stop_training variable\n",
        "    if counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
        "        stop_training = True\n",
        "    if stop_training:\n",
        "        break\n",
        "\n",
        "# Print the last best results and epoch\n",
        "if stop_training:\n",
        "    print(f\"Best RMSE Arousal: {best_rmse_arousal:.4f} at epoch {best_epoch + 1}\")\n",
        "    print(f\"Best MAE Arousal: {best_mae_arousal:.4f} at epoch {best_epoch + 1}\")\n",
        "    print(f\"Best RMSE Valence: {best_rmse_valence:.4f} at epoch {best_epoch + 1}\")\n",
        "    print(f\"Best MAE Valence: {best_mae_valence:.4f} at epoch {best_epoch + 1}\")\n",
        "else:\n",
        "    print(\"Training completed without early stopping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQVkRzmm8R1K"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_on_test(model, test_loader):\n",
        "    model.eval()\n",
        "    y_valence_true = []\n",
        "    y_valence_pred = []\n",
        "    y_arousal_true = []\n",
        "    y_arousal_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            labels_valence = labels[:, 0]\n",
        "            labels_arousal = labels[:, 1]\n",
        "            outputs_valence = outputs[:, 0]\n",
        "            outputs_arousal = outputs[:, 1]\n",
        "\n",
        "            y_valence_true.extend(labels_valence.cpu().numpy().flatten())\n",
        "            y_valence_pred.extend(outputs_valence.cpu().numpy().flatten())\n",
        "            y_arousal_true.extend(labels_arousal.cpu().numpy().flatten())\n",
        "            y_arousal_pred.extend(outputs_arousal.cpu().numpy().flatten())\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae_valence_test = mean_absolute_error(y_valence_true, y_valence_pred)\n",
        "    rmse_valence_test = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
        "    mae_arousal_test = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
        "    rmse_arousal_test = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
        "\n",
        "    return mae_valence_test, rmse_valence_test, mae_arousal_test, rmse_arousal_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load('/content/best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Call the evaluate_on_test function\n",
        "mae_valence_test, rmse_valence_test, mae_arousal_test, rmse_arousal_test = evaluate_on_test(model, test_loader)\n",
        "\n",
        "# Print the results for the test dataset\n",
        "print(f\"Test MAE Valence: {mae_valence_test:.4f}, Test RMSE Valence: {rmse_valence_test:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal_test:.4f}, Test RMSE Arousal: {rmse_arousal_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RsgQQAhOrDK",
        "outputId": "3d8e9a3f-b04e-407b-bd2a-08ea48d03440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE Valence: 0.0743, Test RMSE Valence: 0.0977\n",
            "Test MAE Arousal: 0.0943, Test RMSE Arousal: 0.1355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcq_qy83BFDI"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Only arousal\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
        "from math import sqrt\n",
        "\n",
        "# Load the training dataset\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv\")\n",
        "\n",
        "# Load the dev dataset\n",
        "dev_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv\")\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv\")\n",
        "\n",
        "# Extract features (X) and target (y) for arousal in the training data\n",
        "X_train = train_data.iloc[:, 4:].values  # Features start from column index 4\n",
        "y_arousal_train = train_data[\"arousal\"].values / 10.\n",
        "\n",
        "# Extract features (X) and target (y) for arousal in the dev data\n",
        "X_dev = dev_data.iloc[:, 4:].values\n",
        "y_arousal_dev = dev_data[\"arousal\"].values / 10.\n",
        "\n",
        "# Extract features (X) and target (y) for arousal in the test data\n",
        "X_test = test_data.iloc[:, 4:].values\n",
        "y_arousal_test = test_data[\"arousal\"].values / 10.\n",
        "\n",
        "\n",
        "# Create a parameter grid for SVR\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Concatenate train and dev\n",
        "concat_x_train_dev_arousal = np.concatenate((X_train, X_dev), axis=0)\n",
        "concat_y_arousal_train_dev = np.concatenate((y_arousal_train, y_arousal_dev), axis=0)\n",
        "\n",
        "# Generate indices for training and development parts\n",
        "split_index_arousal = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_dev.shape[0])]\n",
        "\n",
        "# Create PredefinedSplit\n",
        "pds_arousal = PredefinedSplit(test_fold=split_index_arousal)\n",
        "\n",
        "# Use PredefinedSplit in GridSearchCV for Arousal\n",
        "svr_arousal = SVR()\n",
        "grid_search_arousal = GridSearchCV(svr_arousal, param_grid, cv=pds_arousal, scoring='neg_mean_squared_error')\n",
        "grid_search_arousal.fit(concat_x_train_dev_arousal, concat_y_arousal_train_dev)\n",
        "\n",
        "# Get the best estimators\n",
        "best_svr_arousal = grid_search_arousal.best_estimator_\n",
        "\n",
        "# Predict on the dev data for arousal\n",
        "y_arousal_dev_pred = best_svr_arousal.predict(X_dev)\n",
        "\n",
        "# Calculate regression metrics for arousal on the dev data\n",
        "mse_arousal_dev = mean_squared_error(y_arousal_dev, y_arousal_dev_pred)\n",
        "r2_arousal_dev = sqrt(mse_arousal_dev)\n",
        "\n",
        "print(\"Results for Arousal on Dev Data:\")\n",
        "print(\"Arousal - Mean Squared Error:\", mse_arousal_dev)\n",
        "print(\"Arousal - R-squared:\", r2_arousal_dev)\n",
        "\n",
        "# Predict on the test data for arousal\n",
        "y_arousal_test_pred = best_svr_arousal.predict(X_test)\n",
        "\n",
        "# Calculate regression metrics for arousal on the test data\n",
        "mse_arousal_test = mean_squared_error(y_arousal_test, y_arousal_test_pred)\n",
        "r2_arousal_test = sqrt(mse_arousal_test)\n",
        "\n",
        "print(\"Results for Arousal on Test Data:\")\n",
        "print(\"Arousal - Mean Squared Error:\", mse_arousal_test)\n",
        "print(\"Arousal - R-squared:\", r2_arousal_test)"
      ],
      "metadata": {
        "id": "VI1IU6wJQWnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Only valence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
        "from math import sqrt\n",
        "\n",
        "# Load the training dataset\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv\")\n",
        "\n",
        "# Load the dev dataset\n",
        "dev_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv\")\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv\")\n",
        "\n",
        "# Extract features (X) and target (y) for valence in the training data\n",
        "X_train = train_data.iloc[:, 4:].values  # Features start from column index 4\n",
        "y_valence_train = train_data[\"valence\"].values / 10.\n",
        "\n",
        "# Extract features (X) and target (y) for valence in the dev data\n",
        "X_dev = dev_data.iloc[:, 4:].values\n",
        "y_valence_dev = dev_data[\"valence\"].values / 10.\n",
        "\n",
        "# Extract features (X) and target (y) for valence in the test data\n",
        "X_test = test_data.iloc[:, 4:].values\n",
        "y_valence_test = test_data[\"valence\"].values / 10.\n",
        "\n",
        "# Create a parameter grid for SVR\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Concatenate train and dev\n",
        "concat_x_train_dev_valence = np.concatenate((X_train, X_dev), axis=0)\n",
        "concat_y_valence_train_dev = np.concatenate((y_valence_train, y_valence_dev), axis=0)\n",
        "\n",
        "# Generate indices for training and development parts\n",
        "split_index_valence = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_dev.shape[0])]\n",
        "\n",
        "# Create PredefinedSplit\n",
        "pds_valence = PredefinedSplit(test_fold=split_index_valence)\n",
        "\n",
        "# Use PredefinedSplit in GridSearchCV for Valence\n",
        "svr_valence = SVR()\n",
        "grid_search_valence = GridSearchCV(svr_valence, param_grid, cv=pds_valence, scoring='neg_mean_squared_error')\n",
        "grid_search_valence.fit(concat_x_train_dev_valence, concat_y_valence_train_dev)\n",
        "\n",
        "# Get the best estimators\n",
        "best_svr_valence = grid_search_valence.best_estimator_\n",
        "\n",
        "# Predict on the dev data for valence\n",
        "y_valence_dev_pred = best_svr_valence.predict(X_dev)\n",
        "\n",
        "# Calculate regression metrics for valence on the dev data\n",
        "mse_valence_dev = mean_squared_error(y_valence_dev, y_valence_dev_pred)\n",
        "r2_valence_dev = sqrt(mse_valence_dev)\n",
        "\n",
        "print(\"Results for Valence on Dev Data:\")\n",
        "print(\"Valence - Mean Squared Error:\", mse_valence_dev)\n",
        "print(\"Valence - R-squared:\", r2_valence_dev)\n",
        "\n",
        "# Predict on the test data for valence\n",
        "y_valence_test_pred = best_svr_valence.predict(X_test)\n",
        "\n",
        "# Calculate regression metrics for valence on the test data\n",
        "mse_valence_test = mean_squared_error(y_valence_test, y_valence_test_pred)\n",
        "r2_valence_test = sqrt(mse_valence_test)\n",
        "\n",
        "print(\"Results for Valence on Test Data:\")\n",
        "print(\"Valence - Mean Squared Error:\", mse_valence_test)\n",
        "print(\"Valence - R-squared:\", r2_valence_test)"
      ],
      "metadata": {
        "id": "kSl6kjSYQy9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d675ba55-159c-4f6a-a0b4-e81a987e5186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b4fbbd00ded>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the dev dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wci08gnaXJPA",
        "outputId": "bbab426c-b505-4c5c-eaf5-a3802d884481"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-57cc5ae97cf9>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0msvr_valence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mgrid_search_valence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvr_valence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpds_valence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mgrid_search_valence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_x_train_dev_valence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_y_valence_train_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Get the best estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/svm/_libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Both arousal and valence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
        "from math import sqrt\n",
        "\n",
        "# Load the training dataset\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv\")\n",
        "\n",
        "# Load the dev dataset\n",
        "dev_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv\")\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv\")\n",
        "\n",
        "# Extract features (X) and target (y) for arousal and valence in the training data\n",
        "X_train = train_data.iloc[:, 4:].values  # Features start from column index 4\n",
        "y_arousal_train = train_data[\"arousal\"].values / 10.\n",
        "y_valence_train = train_data[\"valence\"].values / 10.\n",
        "\n",
        "# Extract features (X) and target (y) for arousal and valence in the dev data\n",
        "X_dev = dev_data.iloc[:, 4:].values\n",
        "y_arousal_dev = dev_data[\"arousal\"].values / 10.\n",
        "y_valence_dev = dev_data[\"valence\"].values / 10.\n",
        "\n",
        "# Extract features (X) and target (y) for arousal and valence in the test data\n",
        "X_test = test_data.iloc[:, 4:].values\n",
        "y_arousal_test = test_data[\"arousal\"].values / 10.\n",
        "y_valence_test = test_data[\"valence\"].values / 10.\n",
        "\n",
        "# Create a parameter grid for SVR\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Concatenate train and dev\n",
        "concat_x_train_dev_arousal = np.concatenate((X_train, X_dev), axis=0)\n",
        "concat_y_arousal_train_dev = np.concatenate((y_arousal_train, y_arousal_dev), axis=0)\n",
        "\n",
        "concat_x_train_dev_valence = np.concatenate((X_train, X_dev), axis=0)\n",
        "concat_y_valence_train_dev = np.concatenate((y_valence_train, y_valence_dev), axis=0)\n",
        "\n",
        "# Generate indices for training and development parts\n",
        "split_index_arousal = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_dev.shape[0])]\n",
        "split_index_valence = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_dev.shape[0])]\n",
        "\n",
        "# Create PredefinedSplit\n",
        "pds_arousal = PredefinedSplit(test_fold=split_index_arousal)\n",
        "pds_valence = PredefinedSplit(test_fold=split_index_valence)\n",
        "\n",
        "# Use PredefinedSplit in GridSearchCV for Arousal\n",
        "svr_arousal = SVR()\n",
        "grid_search_arousal = GridSearchCV(svr_arousal, param_grid, cv=pds_arousal, scoring='neg_mean_squared_error')\n",
        "grid_search_arousal.fit(concat_x_train_dev_arousal, concat_y_arousal_train_dev)\n",
        "\n",
        "# Use PredefinedSplit in GridSearchCV for Valence\n",
        "svr_valence = SVR()\n",
        "grid_search_valence = GridSearchCV(svr_valence, param_grid, cv=pds_valence, scoring='neg_mean_squared_error')\n",
        "grid_search_valence.fit(concat_x_train_dev_valence, concat_y_valence_train_dev)\n",
        "\n",
        "# Get the best estimators\n",
        "best_svr_arousal = grid_search_arousal.best_estimator_\n",
        "best_svr_valence = grid_search_valence.best_estimator_\n",
        "\n",
        "# Predict on the dev data for arousal\n",
        "y_arousal_dev_pred = best_svr_arousal.predict(X_dev)\n",
        "\n",
        "# Calculate regression metrics for arousal on the dev data\n",
        "mse_arousal_dev = mean_squared_error(y_arousal_dev, y_arousal_dev_pred)\n",
        "r2_arousal_dev = sqrt(mse_arousal_dev)\n",
        "\n",
        "print(\"Results for Arousal on Dev Data:\")\n",
        "print(\"Arousal - Mean Squared Error:\", mse_arousal_dev)\n",
        "print(\"Arousal - R-squared:\", r2_arousal_dev)\n",
        "\n",
        "# Predict on the test data for arousal\n",
        "y_arousal_test_pred = best_svr_arousal.predict(X_test)\n",
        "\n",
        "# Calculate regression metrics for arousal on the test data\n",
        "mse_arousal_test = mean_squared_error(y_arousal_test, y_arousal_test_pred)\n",
        "r2_arousal_test = sqrt(mse_arousal_test)\n",
        "\n",
        "print(\"Results for Arousal on Test Data:\")\n",
        "print(\"Arousal - Mean Squared Error:\", mse_arousal_test)\n",
        "print(\"Arousal - R-squared:\", r2_arousal_test)\n",
        "\n",
        "# Predict on the dev data for valence\n",
        "y_valence_dev_pred = best_svr_valence.predict(X_dev)\n",
        "\n",
        "# Calculate regression metrics for valence on the dev data\n",
        "mse_valence_dev = mean_squared_error(y_valence_dev, y_valence_dev_pred)\n",
        "r2_valence_dev = sqrt(mse_valence_dev)\n",
        "\n",
        "print(\"Results for Valence on Dev Data:\")\n",
        "print(\"Valence - Mean Squared Error:\", mse_valence_dev)\n",
        "print(\"Valence - R-squared:\", r2_valence_dev)\n",
        "\n",
        "# Predict on the test data for valence\n",
        "y_valence_test_pred = best_svr_valence.predict(X_test)\n",
        "\n",
        "# Calculate regression metrics for valence on the test data\n",
        "mse_valence_test = mean_squared_error(y_valence_test, y_valence_test_pred)\n",
        "r2_valence_test = sqrt(mse_valence_test)\n",
        "\n",
        "print(\"Results for Valence on Test Data:\")\n",
        "print(\"Valence - Mean Squared Error:\", mse_valence_test)\n",
        "print(\"Valence - R-squared:\", r2_valence_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA0LwKbzWBoG",
        "outputId": "a426fae8-9f41-4499-f86b-1819c56a7d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "mZaUBpbXV5M_",
        "outputId": "6c907279-05e2-49e4-9165-d20dc86b68db"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-98dae74ffa88>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data/AFEW-VA_radiant_fog_160_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mdev_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data/AFEW-VA_radiant_fog_160_dev.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data/AFEW-VA_radiant_fog_160_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Create instances of CustomVideoDataset for train, dev, and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ],
      "source": [
        "# OLD wrong code\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a custom PyTorch Dataset for video data\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frames = self.video_windows[idx]\n",
        "        print(frames)\n",
        "\n",
        "        # TODO: Extract labels or other information\n",
        "        labels = self.extract_labels(idx)\n",
        "\n",
        "        # Process frames (convert frames to tensors)\n",
        "        frames_tensor = torch.tensor(frames)\n",
        "\n",
        "        return frames_tensor, labels\n",
        "        #labels= tuple (arousal valence)\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            file_path = row['path']\n",
        "            video_id = self.extract_video_info(file_path)\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "            video_frames[video_id].append(file_path)\n",
        "\n",
        "        video_windows = {}\n",
        "        for video_id, frames in video_frames.items():\n",
        "            windows = []\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                window = frames[i:i + self.window_size]\n",
        "                windows.append(window)\n",
        "            video_windows[video_id] = windows\n",
        "\n",
        "        return video_windows\n",
        "        #transforn video_windows to a list\n",
        "        # a = [item for item in video_windows.items()]\n",
        "        # tmp = []\n",
        "        # for item in a:\n",
        "          # for subitem in item:\n",
        "            # tmp = tmp + subitem из листа листов сделать 1 лист\n",
        "        # раньше: 1 картинка, теперь 1 окно!\n",
        "        # след: массив или эррэй с х,у\n",
        "        # по индексу дефинируем ф, потом строим х, у\n",
        "        # ассоциировать фреймы со строчками\n",
        "        # х получается опять массив массивов\n",
        "        # все это происходит в getitem\n",
        "        # можно для удобства запихать в отдельные функции\n",
        "\n",
        "    def extract_labels(self, idx):\n",
        "        #labels = None\n",
        "        return labels\n",
        "\n",
        "# Load your train, dev, and test datasets\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/data/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/data/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/data/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create instances of CustomVideoDataset for train, dev, and test\n",
        "train_dataset = CustomVideoDataset(train_df)\n",
        "dev_dataset = CustomVideoDataset(dev_df)\n",
        "test_dataset = CustomVideoDataset(test_df)\n",
        "\n",
        "# Create DataLoaders for train, dev, and test datasets\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWbSFalNmVqv",
        "outputId": "b2f06ea5-b67c-4fdf-d2ab-338ccdfba9bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 10, 256])\n",
            "torch.Size([32, 2])\n"
          ]
        }
      ],
      "source": [
        "for x,y in train_loader:\n",
        "  print(x.shape) # shape (32, 10, 256)\n",
        "  print(y.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dofGXZOdWCrt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window_frames = self.video_windows[idx]\n",
        "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values.flatten() for frame in window_frames]\n",
        "        # embeddings shape list:(10, 256)\n",
        "        embeddings_flat = np.array(embeddings)\n",
        "        frames_tensor = torch.tensor(embeddings_flat, dtype=torch.float32)\n",
        "\n",
        "        labels = self.labels_windows[idx]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return frames_tensor, labels_tensor\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        labels = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            video_id = self.extract_video_info(row['path'])\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "                labels[video_id] = []\n",
        "            video_frames[video_id].append(row['path'])\n",
        "            labels[video_id].append((row['arousal'], row['valence']))\n",
        "\n",
        "        video_windows = []\n",
        "        labels_windows = []\n",
        "        for video_id in video_frames:\n",
        "            frames = video_frames[video_id]\n",
        "            label_vals = labels[video_id]\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                video_windows.append(frames[i:i + self.window_size])\n",
        "                window_labels = label_vals[i:i + self.window_size]\n",
        "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
        "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
        "                labels_windows.append((avg_arousal, avg_valence))\n",
        "\n",
        "        return video_windows, labels_windows\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "\n",
        "# Load your dataframes\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create instances of CustomVideoDataset for train, dev, and test\n",
        "train_dataset = CustomVideoDataset(train_df)\n",
        "dev_dataset = CustomVideoDataset(dev_df)\n",
        "test_dataset = CustomVideoDataset(test_df)\n",
        "\n",
        "# Create DataLoaders for train, dev, and test datasets\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APB50eeaiHZK"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehH6bPrUWhJA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_on_dev(model, dev_loader):\n",
        "  y_valence_true = []\n",
        "  y_valence_pred = []\n",
        "  y_arousal_true = []\n",
        "  y_arousal_pred = []\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in dev_loader:\n",
        "          outputs = model(inputs)\n",
        "          labels_valence = labels[:, 0]\n",
        "          labels_arousal = labels[:, 1]\n",
        "          outputs_valence = outputs[:, 0]\n",
        "          outputs_arousal = outputs[:, 1]\n",
        "\n",
        "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
        "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
        "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
        "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
        "\n",
        "  # Calculate metrics\n",
        "  mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
        "  rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
        "  mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
        "  rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
        "\n",
        "  return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "MzyfbEnIWvVI",
        "outputId": "5adce9ae-cc22-4c20-9d6a-ecc7d6f4ab43"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (320x256 and 2560x64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f8be398ba27e>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Split predicted values into arousal and valence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f8be398ba27e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (320x256 and 2560x64)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "class FullyConnectedNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FullyConnectedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.tanh(x)  # Apply tanh activation to the output\n",
        "        return x\n",
        "\n",
        "# Define the window size and calculate the input dimension\n",
        "window_size = 10  # Assuming a window size of 10 frames\n",
        "embeddings_per_frame = 256\n",
        "input_dim = window_size * embeddings_per_frame  # 10 frames * 256 embeddings per frame\n",
        "\n",
        "# Define your model and training hyperparameters\n",
        "hidden_dim = 64\n",
        "output_dim = 2  # Arousal and Valence\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Instantiate the model\n",
        "model = FullyConnectedNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define separate loss functions for arousal and valence\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, yhat, y):\n",
        "        return torch.sqrt(self.mse(yhat, y))\n",
        "\n",
        "criterion_arousal = RMSELoss()\n",
        "criterion_valence = RMSELoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define early stopping parameters\n",
        "patience = 10  # Number of epochs to wait for improvement\n",
        "min_val_loss = float('inf')\n",
        "counter = 0  # Counter for epochs without improvement\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "best_mae_arousal = float('inf')\n",
        "best_mae_valence = float('inf')\n",
        "best_rmse_arousal = float('inf')\n",
        "best_rmse_valence = float('inf')\n",
        "stop_training = False\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Split predicted values into arousal and valence\n",
        "        predicted_arousal = outputs[:, 0]\n",
        "        predicted_valence = outputs[:, 1]\n",
        "\n",
        "        # Split ground truth labels into arousal and valence\n",
        "        labels_arousal = labels[:, 0]\n",
        "        labels_valence = labels[:, 1]\n",
        "\n",
        "        # Calculate separate losses for arousal and valence\n",
        "        loss_arousal = criterion_arousal(predicted_arousal, labels_arousal)\n",
        "        loss_valence = criterion_valence(predicted_valence, labels_valence)\n",
        "        loss = loss_arousal + loss_valence\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Predict on dev data using the trained model\n",
        "    model.eval()\n",
        "    dev_mae_valence, dev_rmse_valence, dev_mae_arousal, dev_rmse_arousal = predict_on_dev(model, dev_loader)\n",
        "    general_rmse_metric = (dev_rmse_valence + dev_rmse_arousal) / 2.\n",
        "\n",
        "    print(f\"Validation RMSE: {general_rmse_metric:.4f}\")\n",
        "\n",
        "    # Check Early stopping criteria\n",
        "    if general_rmse_metric < min_val_loss:  # Check if the validation loss has improved\n",
        "      min_val_loss = general_rmse_metric\n",
        "      counter = 0\n",
        "\n",
        "    # Save the model weights if RMSE is lower than the best value\n",
        "      #if dev_rmse_arousal < best_rmse_arousal and dev_rmse_valence < best_rmse_valence:\n",
        "      best_rmse_arousal = dev_rmse_arousal\n",
        "      best_mae_arousal = dev_mae_arousal\n",
        "      best_rmse_valence = dev_rmse_valence\n",
        "      best_mae_valence = dev_mae_valence\n",
        "      best_epoch = epoch\n",
        "      torch.save(model.state_dict(), 'best_model_SEWA.pth') #should be called AFEW actually\n",
        "    else:\n",
        "        counter += 1\n",
        "    # If the validation loss hasn't improved for 'patience' epochs, set the stop_training variable\n",
        "    if counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
        "        stop_training = True\n",
        "    if stop_training:\n",
        "        break\n",
        "\n",
        "# Print the last best results and epoch\n",
        "if stop_training:\n",
        "    print(f\"Best RMSE Arousal: {best_rmse_arousal:.4f} at epoch {best_epoch + 1}\")\n",
        "    print(f\"Best MAE Arousal: {best_mae_arousal:.4f} at epoch {best_epoch + 1}\")\n",
        "    print(f\"Best RMSE Valence: {best_rmse_valence:.4f} at epoch {best_epoch + 1}\")\n",
        "    print(f\"Best MAE Valence: {best_mae_valence:.4f} at epoch {best_epoch + 1}\")\n",
        "else:\n",
        "    print(\"Training completed without early stopping\")\n",
        "\n",
        "def evaluate_on_test(model, test_loader):\n",
        "    model.eval()\n",
        "    y_valence_true = []\n",
        "    y_valence_pred = []\n",
        "    y_arousal_true = []\n",
        "    y_arousal_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            labels_valence = labels[:, 0]\n",
        "            labels_arousal = labels[:, 1]\n",
        "            outputs_valence = outputs[:, 0]\n",
        "            outputs_arousal = outputs[:, 1]\n",
        "\n",
        "            y_valence_true.extend(labels_valence.cpu().numpy().flatten())\n",
        "            y_valence_pred.extend(outputs_valence.cpu().numpy().flatten())\n",
        "            y_arousal_true.extend(labels_arousal.cpu().numpy().flatten())\n",
        "            y_arousal_pred.extend(outputs_arousal.cpu().numpy().flatten())\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae_valence_test = mean_absolute_error(y_valence_true, y_valence_pred)\n",
        "    rmse_valence_test = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
        "    mae_arousal_test = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
        "    rmse_arousal_test = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
        "\n",
        "    return mae_valence_test, rmse_valence_test, mae_arousal_test, rmse_arousal_test\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('/content/best_model_SEWA.pth')) #AFEW actually\n",
        "model.eval()\n",
        "\n",
        "# Call the evaluate_on_test function\n",
        "mae_valence_test, rmse_valence_test, mae_arousal_test, rmse_arousal_test = evaluate_on_test(model, test_loader)\n",
        "\n",
        "# Print the results for the test dataset\n",
        "print(f\"Test MAE Valence: {mae_valence_test:.4f}, Test RMSE Valence: {rmse_valence_test:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal_test:.4f}, Test RMSE Arousal: {rmse_arousal_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVo9wYOfp7us",
        "outputId": "03425771-6606-4301-e6dc-20bfeb6bd4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Validation Loss: 9.1029\n",
            "Epoch 2/10, Validation Loss: 9.1272\n",
            "Epoch 3/10, Validation Loss: 9.1784\n",
            "Epoch 4/10, Validation Loss: 9.1269\n",
            "Epoch 5/10, Validation Loss: 9.1227\n",
            "Epoch 6/10, Validation Loss: 9.1000\n",
            "Epoch 7/10, Validation Loss: 9.1187\n",
            "Epoch 8/10, Validation Loss: 9.1081\n",
            "Epoch 9/10, Validation Loss: 9.1686\n",
            "Epoch 10/10, Validation Loss: 9.0963\n",
            "Test MAE Valence: 2.1868, Test RMSE Valence: 2.9043\n",
            "Test MAE Arousal: 1.9744, Test RMSE Arousal: 2.7255\n"
          ]
        }
      ],
      "source": [
        "#LSTM:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window_frames = self.video_windows[idx]\n",
        "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
        "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
        "\n",
        "        labels = self.labels_windows[idx]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return frames_tensor, labels_tensor\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        labels = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            video_id = self.extract_video_info(row['path'])\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "                labels[video_id] = []\n",
        "            video_frames[video_id].append(row['path'])\n",
        "            labels[video_id].append((row['arousal'], row['valence']))\n",
        "\n",
        "        video_windows = []\n",
        "        labels_windows = []\n",
        "        for video_id in video_frames:\n",
        "            frames = video_frames[video_id]\n",
        "            label_vals = labels[video_id]\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                video_windows.append(frames[i:i + self.window_size])\n",
        "                window_labels = label_vals[i:i + self.window_size]\n",
        "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
        "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
        "                labels_windows.append((avg_arousal, avg_valence))\n",
        "\n",
        "        return video_windows, labels_windows\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "\n",
        "# LSTM Network\n",
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # GRU instead of LSTM\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # for 3. approach conv1d_layer = torch.nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=seq_length)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # TODO: delete it\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # TODO: delete it\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # We try several models\n",
        "        # 1. just take last state (out[:, -1, :])\n",
        "        # 2. Adaptive AVG or MAX pooling (https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)\n",
        "        # 3. 1D CNN layer (https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
        "\n",
        "        # for 3. approach\n",
        "        # torch.permute(out, (0, 2, 1)) (batch_size, seq_length, hidden_size) -> (batch_size, hidden_size, seq_length)\n",
        "        # out = conv1d_layer(out)\n",
        "        # out = out.squeeze()\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "window_size = 10\n",
        "input_size = 256  # Number of features (embeddings) per frame\n",
        "hidden_size = 128  # Number of features in hidden state of LSTM\n",
        "output_size = 2  # Output size (arousal and valence)\n",
        "num_layers = 1  # Number of LSTM layers\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CustomVideoDataset(train_df, window_size)\n",
        "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
        "test_dataset = CustomVideoDataset(test_df, window_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model\n",
        "model = LSTMNetwork(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            y_true.append(labels.numpy())\n",
        "            y_pred.append(outputs.numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
        "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
        "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
        "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
        "\n",
        "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmqPuWUyN4Gf",
        "outputId": "213ff55b-2787-46e4-eee7-f86eaa4e2b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Validation Loss: 9.0849\n",
            "Epoch 2/10, Validation Loss: 9.0904\n",
            "Epoch 3/10, Validation Loss: 9.0964\n",
            "Epoch 4/10, Validation Loss: 9.1111\n",
            "Epoch 5/10, Validation Loss: 9.1322\n",
            "Epoch 6/10, Validation Loss: 9.1282\n",
            "Epoch 7/10, Validation Loss: 9.1163\n",
            "Epoch 8/10, Validation Loss: 9.1390\n",
            "Epoch 9/10, Validation Loss: 9.1170\n",
            "Epoch 10/10, Validation Loss: 9.1371\n",
            "Test MAE Valence: 2.2106, Test RMSE Valence: 2.9149\n",
            "Test MAE Arousal: 2.0104, Test RMSE Arousal: 2.7364\n"
          ]
        }
      ],
      "source": [
        "#GRU instead LSTM, last state approach\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window_frames = self.video_windows[idx]\n",
        "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
        "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
        "\n",
        "        labels = self.labels_windows[idx]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return frames_tensor, labels_tensor\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        labels = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            video_id = self.extract_video_info(row['path'])\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "                labels[video_id] = []\n",
        "            video_frames[video_id].append(row['path'])\n",
        "            labels[video_id].append((row['arousal'], row['valence']))\n",
        "\n",
        "        video_windows = []\n",
        "        labels_windows = []\n",
        "        for video_id in video_frames:\n",
        "            frames = video_frames[video_id]\n",
        "            label_vals = labels[video_id]\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                video_windows.append(frames[i:i + self.window_size])\n",
        "                window_labels = label_vals[i:i + self.window_size]\n",
        "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
        "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
        "                labels_windows.append((avg_arousal, avg_valence))\n",
        "\n",
        "        return video_windows, labels_windows\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "\n",
        "# GRU Network\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(GRUNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # GRU Layer\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "window_size = 10\n",
        "input_size = 256  # Number of features (embeddings) per frame\n",
        "hidden_size = 128  # Number of features in hidden state of GRU\n",
        "output_size = 2  # Output size (arousal and valence)\n",
        "num_layers = 1  # Number of GRU layers\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CustomVideoDataset(train_df, window_size)\n",
        "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
        "test_dataset = CustomVideoDataset(test_df, window_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model with GRU\n",
        "model = GRUNetwork(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Test evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            y_true.append(labels.numpy())\n",
        "            y_pred.append(outputs.numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
        "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
        "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
        "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
        "\n",
        "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU, AVG Pooling approach\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window_frames = self.video_windows[idx]\n",
        "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
        "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
        "\n",
        "        labels = self.labels_windows[idx]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return frames_tensor, labels_tensor\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        labels = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            video_id = self.extract_video_info(row['path'])\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "                labels[video_id] = []\n",
        "            video_frames[video_id].append(row['path'])\n",
        "            labels[video_id].append((row['arousal'], row['valence']))\n",
        "\n",
        "        video_windows = []\n",
        "        labels_windows = []\n",
        "        for video_id in video_frames:\n",
        "            frames = video_frames[video_id]\n",
        "            label_vals = labels[video_id]\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                video_windows.append(frames[i:i + self.window_size])\n",
        "                window_labels = label_vals[i:i + self.window_size]\n",
        "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
        "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
        "                labels_windows.append((avg_arousal, avg_valence))\n",
        "\n",
        "        return video_windows, labels_windows\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, seq_length=10):\n",
        "        super(GRUNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # GRU Layer\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        #self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, hidden_size))# Adaptive Avg Pooling\n",
        "        # 1D Average Pooling\n",
        "        self.avg_pool1d = nn.AvgPool1d(seq_length)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Apply 1D average pooling\n",
        "        out = out.permute(0, 2, 1)  # Change shape to (batch_size, hidden_size, seq_length) for AvgPool1d\n",
        "        out = self.avg_pool1d(out)\n",
        "        out = out.squeeze(2)  # Remove the last dimension\n",
        "\n",
        "        # Decode the hidden state\n",
        "        out = self.fc(out)\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "window_size = 10\n",
        "input_size = 256  # Number of features (embeddings) per frame\n",
        "hidden_size = 128  # Number of features in hidden state of GRU\n",
        "output_size = 2  # Output size (arousal and valence)\n",
        "num_layers = 1  # Number of GRU layers\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CustomVideoDataset(train_df, window_size)\n",
        "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
        "test_dataset = CustomVideoDataset(test_df, window_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model with GRU\n",
        "model = GRUNetwork(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Test evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            y_true.append(labels.numpy())\n",
        "            y_pred.append(outputs.numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
        "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
        "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
        "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
        "\n",
        "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_gq6-pE5UIE",
        "outputId": "bfad08a0-d476-46ed-b44f-6d6fe28852b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f5082fb22f33>:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Validation Loss: 9.1426\n",
            "Epoch 2/10, Validation Loss: 9.1051\n",
            "Epoch 3/10, Validation Loss: 9.1180\n",
            "Epoch 4/10, Validation Loss: 9.1184\n",
            "Epoch 5/10, Validation Loss: 9.1250\n",
            "Epoch 6/10, Validation Loss: 9.1272\n",
            "Epoch 7/10, Validation Loss: 9.2115\n",
            "Epoch 8/10, Validation Loss: 9.1271\n",
            "Epoch 9/10, Validation Loss: 9.1267\n",
            "Epoch 10/10, Validation Loss: 9.1150\n",
            "Test MAE Valence: 2.2398, Test RMSE Valence: 2.9325\n",
            "Test MAE Arousal: 1.9690, Test RMSE Arousal: 2.7247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, seq_length=10):\n",
        "        super(GRUNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # GRU Layer\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        #self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, hidden_size))# Adaptive Avg Pooling\n",
        "        # 1D Average Pooling\n",
        "        self.avg_pool1d = nn.AvgPool1d(seq_length)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Apply 1D average pooling\n",
        "        print(out.shape)\n",
        "        out = out.permute(0, 2, 1)  # Change shape to (batch_size, hidden_size, seq_length) for AvgPool1d\n",
        "        out = self.avg_pool1d(out)\n",
        "        out = out.squeeze(2)  # Remove the last dimension\n",
        "        print(out.shape)\n",
        "\n",
        "        # Decode the hidden state\n",
        "        out = self.fc(out)\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "# Hyperparameters\n",
        "window_size = 10\n",
        "input_size = 256  # Number of features (embeddings) per frame\n",
        "hidden_size = 128  # Number of features in hidden state of GRU\n",
        "output_size = 2  # Output size (arousal and valence)\n",
        "num_layers = 1  # Number of GRU layers\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "# Initialize the model with GRU\n",
        "model = GRUNetwork(input_size, hidden_size, output_size, num_layers)\n",
        "summary(model, (10,256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-BoleueoPyw",
        "outputId": "4c59cc71-9542-4038-a294-3dc6a3fbf85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 256])\n",
            "torch.Size([2, 10, 128])\n",
            "torch.Size([2, 128])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "               GRU-1  [[-1, 10, 128], [-1, 2, 128]]               0\n",
            "         AvgPool1d-2               [-1, 128, 1]               0\n",
            "            Linear-3                    [-1, 2]             258\n",
            "              Tanh-4                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 258\n",
            "Trainable params: 258\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.50\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 2.51\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GRU, MAX Pooling approach\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window_frames = self.video_windows[idx]\n",
        "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
        "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
        "\n",
        "        labels = self.labels_windows[idx]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return frames_tensor, labels_tensor\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        labels = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            video_id = self.extract_video_info(row['path'])\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "                labels[video_id] = []\n",
        "            video_frames[video_id].append(row['path'])\n",
        "            labels[video_id].append((row['arousal'], row['valence']))\n",
        "\n",
        "        video_windows = []\n",
        "        labels_windows = []\n",
        "        for video_id in video_frames:\n",
        "            frames = video_frames[video_id]\n",
        "            label_vals = labels[video_id]\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                video_windows.append(frames[i:i + self.window_size])\n",
        "                window_labels = label_vals[i:i + self.window_size]\n",
        "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
        "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
        "                labels_windows.append((avg_arousal, avg_valence))\n",
        "\n",
        "        return video_windows, labels_windows\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(GRUNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # GRU Layer\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Max Pooling\n",
        "        self.max_pool = nn.MaxPool2d((1, hidden_size))\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Apply max pooling\n",
        "        out = out.unsqueeze(1)  # Add a channel dimension for pooling\n",
        "        out = self.max_pool(out)\n",
        "        out = out.squeeze()  # Remove extra dimensions\n",
        "\n",
        "        # Decode the hidden state\n",
        "        out = self.fc(out)\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "window_size = 10\n",
        "input_size = 256  # Number of features (embeddings) per frame\n",
        "hidden_size = 128  # Number of features in hidden state of GRU\n",
        "output_size = 2  # Output size (arousal and valence)\n",
        "num_layers = 1  # Number of GRU layers\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CustomVideoDataset(train_df, window_size)\n",
        "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
        "test_dataset = CustomVideoDataset(test_df, window_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model with GRU\n",
        "model = GRUNetwork(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Test evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            y_true.append(labels.numpy())\n",
        "            y_pred.append(outputs.numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
        "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
        "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
        "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
        "\n",
        "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "uaST9lFh6BBT",
        "outputId": "45fff0fb-f4da-43ef-a99d-2d6a18b65465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32x10 and 128x2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-420075ab38bc>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-420075ab38bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Decode the hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x10 and 128x2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GRU, 1D CNN Layer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomVideoDataset(Dataset):\n",
        "    def __init__(self, df, window_size=10, stride=5):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window_frames = self.video_windows[idx]\n",
        "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
        "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
        "\n",
        "        labels = self.labels_windows[idx]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return frames_tensor, labels_tensor\n",
        "\n",
        "    def prepare_windows(self):\n",
        "        video_frames = {}\n",
        "        labels = {}\n",
        "        for _, row in self.df.iterrows():\n",
        "            video_id = self.extract_video_info(row['path'])\n",
        "            if video_id not in video_frames:\n",
        "                video_frames[video_id] = []\n",
        "                labels[video_id] = []\n",
        "            video_frames[video_id].append(row['path'])\n",
        "            labels[video_id].append((row['arousal'], row['valence']))\n",
        "\n",
        "        video_windows = []\n",
        "        labels_windows = []\n",
        "        for video_id in video_frames:\n",
        "            frames = video_frames[video_id]\n",
        "            label_vals = labels[video_id]\n",
        "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
        "                video_windows.append(frames[i:i + self.window_size])\n",
        "                window_labels = label_vals[i:i + self.window_size]\n",
        "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
        "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
        "                labels_windows.append((avg_arousal, avg_valence))\n",
        "\n",
        "        return video_windows, labels_windows\n",
        "\n",
        "    def extract_video_info(self, file_path):\n",
        "        parts = file_path.split('/')\n",
        "        video_id = parts[-2]\n",
        "        return video_id\n",
        "\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, seq_length=10):\n",
        "        super(GRUNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # GRU Layer\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # 1D CNN Layer\n",
        "        self.conv1d_layer = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=seq_length)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Apply 1D CNN\n",
        "        out = out.permute(0, 2, 1)  # Change shape to (batch_size, hidden_size, seq_length) for Conv1d\n",
        "        out = self.conv1d_layer(out)\n",
        "        out = out.squeeze(2)  # Remove the last dimension\n",
        "\n",
        "        # Decode the hidden state\n",
        "        out = self.fc(out)\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "window_size = 10\n",
        "input_size = 256  # Number of features (embeddings) per frame\n",
        "hidden_size = 128  # Number of features in hidden state of GRU\n",
        "output_size = 2  # Output size (arousal and valence)\n",
        "num_layers = 1  # Number of GRU layers\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/AFEW-VA_radiant_fog_160_test.csv')\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CustomVideoDataset(train_df, window_size)\n",
        "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
        "test_dataset = CustomVideoDataset(test_df, window_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model with GRU\n",
        "model = GRUNetwork(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Test evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            y_true.append(labels.numpy())\n",
        "            y_pred.append(outputs.numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
        "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
        "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
        "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
        "\n",
        "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
        "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv9-QUx78BMn",
        "outputId": "03f199a7-9b34-4196-f562-45c484e50d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-14f02bfa2486>:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Validation Loss: 9.1674\n",
            "Epoch 2/10, Validation Loss: 9.1590\n",
            "Epoch 3/10, Validation Loss: 9.1049\n",
            "Epoch 4/10, Validation Loss: 9.1288\n",
            "Epoch 5/10, Validation Loss: 9.1284\n",
            "Epoch 6/10, Validation Loss: 9.1222\n",
            "Epoch 7/10, Validation Loss: 9.1142\n",
            "Epoch 8/10, Validation Loss: 9.1572\n",
            "Epoch 9/10, Validation Loss: 9.1566\n",
            "Epoch 10/10, Validation Loss: 9.1251\n",
            "Test MAE Valence: 2.2061, Test RMSE Valence: 2.9216\n",
            "Test MAE Arousal: 1.9929, Test RMSE Arousal: 2.7300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ibf8AXurBbw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
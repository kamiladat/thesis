{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfad5e2-765f-4a06-9552-f5473125abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1376\n",
      "Epoch 1/100, Validation Loss: 0.1076\n",
      "Model saved to sewa-best_GRU_AVG-5.pth\n",
      "Epoch 2/100, Training Loss: 0.1205\n",
      "Epoch 2/100, Validation Loss: 0.1144\n",
      "Epoch 3/100, Training Loss: 0.1156\n",
      "Epoch 3/100, Validation Loss: 0.1107\n",
      "Epoch 4/100, Training Loss: 0.1158\n",
      "Epoch 4/100, Validation Loss: 0.1080\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.1031\n",
      "Epoch 5/100, Validation Loss: 0.1023\n",
      "Model saved to sewa-best_GRU_AVG-5.pth\n",
      "Epoch 6/100, Training Loss: 0.1014\n",
      "Epoch 6/100, Validation Loss: 0.1032\n",
      "Epoch 7/100, Training Loss: 0.1004\n",
      "Epoch 7/100, Validation Loss: 0.1027\n",
      "Epoch 8/100, Training Loss: 0.0990\n",
      "Epoch 8/100, Validation Loss: 0.1019\n",
      "Model saved to sewa-best_GRU_AVG-5.pth\n",
      "Epoch 9/100, Training Loss: 0.0979\n",
      "Epoch 9/100, Validation Loss: 0.1034\n",
      "Epoch 10/100, Training Loss: 0.0974\n",
      "Epoch 10/100, Validation Loss: 0.1028\n",
      "Epoch 11/100, Training Loss: 0.0960\n",
      "Epoch 11/100, Validation Loss: 0.1027\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 12/100, Training Loss: 0.0943\n",
      "Epoch 12/100, Validation Loss: 0.1048\n",
      "Epoch 13/100, Training Loss: 0.0937\n",
      "Epoch 13/100, Validation Loss: 0.1048\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0906, Test RMSE Valence: 0.1287\n",
      "Test MAE Arousal: 0.0698, Test RMSE Arousal: 0.0887\n"
     ]
    }
   ],
   "source": [
    "#GRU avg pool WITH GPU CPU SWITCH, sewa 5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 5\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.5):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Directly average across the sequence length dimension\n",
    "        out = torch.mean(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_AVG-5.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d962f236-5246-4453-ba66-8eb0ac27178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1352\n",
      "Epoch 1/100, Validation Loss: 0.1139\n",
      "Model saved to sewa-best_GRU_AVG-10.pth\n",
      "Epoch 2/100, Training Loss: 0.1136\n",
      "Epoch 2/100, Validation Loss: 0.1096\n",
      "Model saved to sewa-best_GRU_AVG-10.pth\n",
      "Epoch 3/100, Training Loss: 0.1107\n",
      "Epoch 3/100, Validation Loss: 0.1004\n",
      "Model saved to sewa-best_GRU_AVG-10.pth\n",
      "Epoch 4/100, Training Loss: 0.1086\n",
      "Epoch 4/100, Validation Loss: 0.1054\n",
      "Epoch 5/100, Training Loss: 0.1038\n",
      "Epoch 5/100, Validation Loss: 0.1081\n",
      "Epoch 6/100, Training Loss: 0.1020\n",
      "Epoch 6/100, Validation Loss: 0.1025\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 7/100, Training Loss: 0.0903\n",
      "Epoch 7/100, Validation Loss: 0.0972\n",
      "Model saved to sewa-best_GRU_AVG-10.pth\n",
      "Epoch 8/100, Training Loss: 0.0875\n",
      "Epoch 8/100, Validation Loss: 0.1023\n",
      "Epoch 9/100, Training Loss: 0.0854\n",
      "Epoch 9/100, Validation Loss: 0.1026\n",
      "Epoch 10/100, Training Loss: 0.0847\n",
      "Epoch 10/100, Validation Loss: 0.1055\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 11/100, Training Loss: 0.0820\n",
      "Epoch 11/100, Validation Loss: 0.1056\n",
      "Epoch 12/100, Training Loss: 0.0817\n",
      "Epoch 12/100, Validation Loss: 0.1057\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0940, Test RMSE Valence: 0.1317\n",
      "Test MAE Arousal: 0.0761, Test RMSE Arousal: 0.0948\n"
     ]
    }
   ],
   "source": [
    "#GRU avg pool WITH GPU CPU SWITCH, sewa 10 //TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 10\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Directly average across the sequence length dimension\n",
    "        out = torch.mean(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_AVG-10.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a887e847-1343-4ac9-a199-2cbf22203790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24939/443460802.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence - Dev MSE: 0.015723829621847926 Dev RMSE: 0.12539469534971534 Test MSE: 0.02818249009312014 Test RMSE: 0.1678764131530101\n"
     ]
    }
   ],
   "source": [
    "#1 sec only val\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_valence = data['valence'].values\n",
    "    return X, y_valence\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"1sec/SEWA_features_wav2vec_1_seconds_train.csv\"\n",
    "dev_file = \"1sec/SEWA_features_wav2vec_1_seconds_dev.csv\"\n",
    "test_file = \"1sec/SEWA_features_wav2vec_1_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_valence_train = load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_valence_dev = load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_valence_test = load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Valence Model\n",
    "best_svr_valence = svr_grid_search(X_train_scaled, y_valence_train, X_dev_scaled, y_valence_dev, param_grid)\n",
    "mse_valence_dev, rmse_valence_dev, mse_valence_test, rmse_valence_test = evaluate_model(best_svr_valence, X_dev_scaled, y_valence_dev, X_test_scaled, y_valence_test)\n",
    "\n",
    "# Results\n",
    "print(\"Valence - Dev MSE:\", mse_valence_dev, \"Dev RMSE:\", rmse_valence_dev, \"Test MSE:\", mse_valence_test, \"Test RMSE:\", rmse_valence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51dc729-da6b-43d0-95ab-a63619ec368f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arousal - Dev MSE: 0.01150712431384633 Dev RMSE: 0.10727126508924154 Test MSE: 0.0337811262872568 Test RMSE: 0.18379642620915349\n"
     ]
    }
   ],
   "source": [
    "#2 sec ar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_arousal = data['arousal'].values\n",
    "    return X, y_arousal\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"2sec/SEWA_features_wav2vec_2_seconds_train.csv\"\n",
    "dev_file = \"2sec/SEWA_features_wav2vec_2_seconds_dev.csv\"\n",
    "test_file = \"2sec/SEWA_features_wav2vec_2_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_arousal_train= load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_arousal_dev= load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_arousal_test= load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Arousal Model\n",
    "best_svr_arousal = svr_grid_search(X_train_scaled, y_arousal_train, X_dev_scaled, y_arousal_dev, param_grid)\n",
    "mse_arousal_dev, rmse_arousal_dev, mse_arousal_test, rmse_arousal_test = evaluate_model(best_svr_arousal, X_dev_scaled, y_arousal_dev, X_test_scaled, y_arousal_test)\n",
    "\n",
    "# Results\n",
    "print(\"Arousal - Dev MSE:\", mse_arousal_dev, \"Dev RMSE:\", rmse_arousal_dev, \"Test MSE:\", mse_arousal_test, \"Test RMSE:\", rmse_arousal_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a37eef-c376-4688-990d-83e13015db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence - Dev MSE: 0.014963897433684464 Dev RMSE: 0.12232701023765954 Test MSE: 0.028525227667764635 Test RMSE: 0.1688941315373765\n"
     ]
    }
   ],
   "source": [
    "#2 sec val\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_valence = data['valence'].values\n",
    "    return X, y_valence\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"2sec/SEWA_features_wav2vec_2_seconds_train.csv\"\n",
    "dev_file = \"2sec/SEWA_features_wav2vec_2_seconds_dev.csv\"\n",
    "test_file = \"2sec/SEWA_features_wav2vec_2_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_valence_train = load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_valence_dev = load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_valence_test = load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Valence Model\n",
    "best_svr_valence = svr_grid_search(X_train_scaled, y_valence_train, X_dev_scaled, y_valence_dev, param_grid)\n",
    "mse_valence_dev, rmse_valence_dev, mse_valence_test, rmse_valence_test = evaluate_model(best_svr_valence, X_dev_scaled, y_valence_dev, X_test_scaled, y_valence_test)\n",
    "\n",
    "# Results\n",
    "print(\"Valence - Dev MSE:\", mse_valence_dev, \"Dev RMSE:\", rmse_valence_dev, \"Test MSE:\", mse_valence_test, \"Test RMSE:\", rmse_valence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0264a7-0dfc-4ab6-8171-7006b7cca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arousal - Dev MSE: 0.011329502120488008 Dev RMSE: 0.106440133974399 Test MSE: 0.03427857801002751 Test RMSE: 0.18514474880489457\n"
     ]
    }
   ],
   "source": [
    "#3 sec ar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_arousal = data['arousal'].values\n",
    "    return X, y_arousal\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"3sec/SEWA_features_wav2vec_3_seconds_train.csv\"\n",
    "dev_file = \"3sec/SEWA_features_wav2vec_3_seconds_dev.csv\"\n",
    "test_file = \"3sec/SEWA_features_wav2vec_3_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_arousal_train= load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_arousal_dev= load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_arousal_test= load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Arousal Model\n",
    "best_svr_arousal = svr_grid_search(X_train_scaled, y_arousal_train, X_dev_scaled, y_arousal_dev, param_grid)\n",
    "mse_arousal_dev, rmse_arousal_dev, mse_arousal_test, rmse_arousal_test = evaluate_model(best_svr_arousal, X_dev_scaled, y_arousal_dev, X_test_scaled, y_arousal_test)\n",
    "\n",
    "# Results\n",
    "print(\"Arousal - Dev MSE:\", mse_arousal_dev, \"Dev RMSE:\", rmse_arousal_dev, \"Test MSE:\", mse_arousal_test, \"Test RMSE:\", rmse_arousal_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec56e67-40fe-4487-b8a8-c88375daacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence - Dev MSE: 0.014270796036210186 Dev RMSE: 0.11946043711710663 Test MSE: 0.02775016039644333 Test RMSE: 0.1665837939189864\n"
     ]
    }
   ],
   "source": [
    "#3 sec val\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_valence = data['valence'].values\n",
    "    return X, y_valence\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"3sec/SEWA_features_wav2vec_3_seconds_train.csv\"\n",
    "dev_file = \"3sec/SEWA_features_wav2vec_3_seconds_dev.csv\"\n",
    "test_file = \"3sec/SEWA_features_wav2vec_3_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_valence_train = load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_valence_dev = load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_valence_test = load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Valence Model\n",
    "best_svr_valence = svr_grid_search(X_train_scaled, y_valence_train, X_dev_scaled, y_valence_dev, param_grid)\n",
    "mse_valence_dev, rmse_valence_dev, mse_valence_test, rmse_valence_test = evaluate_model(best_svr_valence, X_dev_scaled, y_valence_dev, X_test_scaled, y_valence_test)\n",
    "\n",
    "# Results\n",
    "print(\"Valence - Dev MSE:\", mse_valence_dev, \"Dev RMSE:\", rmse_valence_dev, \"Test MSE:\", mse_valence_test, \"Test RMSE:\", rmse_valence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706df596-b5c8-450d-b658-d0372901c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arousal - Dev MSE: 0.011034525287948183 Dev RMSE: 0.10504534872115082 Test MSE: 0.033976497371320497 Test RMSE: 0.18432714767857852\n"
     ]
    }
   ],
   "source": [
    "#4 sec ar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_arousal = data['arousal'].values\n",
    "    return X, y_arousal\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"4sec/SEWA_features_wav2vec_4_seconds_train.csv\"\n",
    "dev_file = \"4sec/SEWA_features_wav2vec_4_seconds_dev.csv\"\n",
    "test_file = \"4sec/SEWA_features_wav2vec_4_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_arousal_train= load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_arousal_dev= load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_arousal_test= load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Arousal Model\n",
    "best_svr_arousal = svr_grid_search(X_train_scaled, y_arousal_train, X_dev_scaled, y_arousal_dev, param_grid)\n",
    "mse_arousal_dev, rmse_arousal_dev, mse_arousal_test, rmse_arousal_test = evaluate_model(best_svr_arousal, X_dev_scaled, y_arousal_dev, X_test_scaled, y_arousal_test)\n",
    "\n",
    "# Results\n",
    "print(\"Arousal - Dev MSE:\", mse_arousal_dev, \"Dev RMSE:\", rmse_arousal_dev, \"Test MSE:\", mse_arousal_test, \"Test RMSE:\", rmse_arousal_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74043031-3f39-4f23-8509-1f2dc061c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence - Dev MSE: 0.014388082103504183 Dev RMSE: 0.1199503318190666 Test MSE: 0.028316769121122524 Test RMSE: 0.1682758720706047\n"
     ]
    }
   ],
   "source": [
    "#4 sec val\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_dataset(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    features_start_col = data.columns.get_loc(\"x_0\")\n",
    "    X = data.iloc[:, features_start_col:].values  # Adjusted to slice till the end\n",
    "    y_valence = data['valence'].values\n",
    "    return X, y_valence\n",
    "\n",
    "# Scale features (function)\n",
    "def scale_features(X_train, X_dev, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_dev_scaled, X_test_scaled\n",
    "\n",
    "# SVR Grid Search (function)\n",
    "def svr_grid_search(X_train, y_train, X_dev, y_dev, param_grid):\n",
    "    concat_x_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
    "    concat_y_train_dev = np.concatenate((y_train, y_dev), axis=0)\n",
    "    split_index = [-1 for _ in X_train] + [0 for _ in X_dev]  # PredefinedSplit indices\n",
    "    pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "    svr = SVR()\n",
    "    grid_search = GridSearchCV(svr, param_grid, cv=pds, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(concat_x_train_dev, concat_y_train_dev)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Model (function)\n",
    "def evaluate_model(model, X_dev, y_dev, X_test, y_test):\n",
    "    # Dev set\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    mse_dev = mean_squared_error(y_dev, y_dev_pred)\n",
    "    rmse_dev = sqrt(mse_dev)\n",
    "    # Test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = sqrt(mse_test)\n",
    "    return mse_dev, rmse_dev, mse_test, rmse_test\n",
    "\n",
    "# Paths to datasets\n",
    "train_file = \"4sec/SEWA_features_wav2vec_4_seconds_train.csv\"\n",
    "dev_file = \"4sec/SEWA_features_wav2vec_4_seconds_dev.csv\"\n",
    "test_file = \"4sec/SEWA_features_wav2vec_4_seconds_test.csv\"\n",
    "\n",
    "# Load and preprocess datasets\n",
    "X_train, y_valence_train = load_and_preprocess_dataset(train_file)\n",
    "X_dev, y_valence_dev = load_and_preprocess_dataset(dev_file)\n",
    "X_test, y_valence_test = load_and_preprocess_dataset(test_file)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_dev_scaled, X_test_scaled = scale_features(X_train, X_dev, X_test)\n",
    "\n",
    "# SVR parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Valence Model\n",
    "best_svr_valence = svr_grid_search(X_train_scaled, y_valence_train, X_dev_scaled, y_valence_dev, param_grid)\n",
    "mse_valence_dev, rmse_valence_dev, mse_valence_test, rmse_valence_test = evaluate_model(best_svr_valence, X_dev_scaled, y_valence_dev, X_test_scaled, y_valence_test)\n",
    "\n",
    "# Results\n",
    "print(\"Valence - Dev MSE:\", mse_valence_dev, \"Dev RMSE:\", rmse_valence_dev, \"Test MSE:\", mse_valence_test, \"Test RMSE:\", rmse_valence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bdaf24-536e-4167-a92c-07827dcbe637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13949/2376359720.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/tmp/ipykernel_13949/2376359720.py:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343970094/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1400\n",
      "Epoch 1/100, Validation Loss: 0.0960\n",
      "Model saved to sewa-best_GRU_AVG-15.pth\n",
      "Epoch 2/100, Training Loss: 0.1131\n",
      "Epoch 2/100, Validation Loss: 0.1092\n",
      "Epoch 3/100, Training Loss: 0.1096\n",
      "Epoch 3/100, Validation Loss: 0.1127\n",
      "Epoch 4/100, Training Loss: 0.1070\n",
      "Epoch 4/100, Validation Loss: 0.1250\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.0942\n",
      "Epoch 5/100, Validation Loss: 0.0956\n",
      "Model saved to sewa-best_GRU_AVG-15.pth\n",
      "Epoch 6/100, Training Loss: 0.0891\n",
      "Epoch 6/100, Validation Loss: 0.0950\n",
      "Model saved to sewa-best_GRU_AVG-15.pth\n",
      "Epoch 7/100, Training Loss: 0.0874\n",
      "Epoch 7/100, Validation Loss: 0.0955\n",
      "Epoch 8/100, Training Loss: 0.0860\n",
      "Epoch 8/100, Validation Loss: 0.0956\n",
      "Epoch 9/100, Training Loss: 0.0844\n",
      "Epoch 9/100, Validation Loss: 0.0965\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 10/100, Training Loss: 0.0810\n",
      "Epoch 10/100, Validation Loss: 0.0972\n",
      "Epoch 11/100, Training Loss: 0.0803\n",
      "Epoch 11/100, Validation Loss: 0.0978\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0894, Test RMSE Valence: 0.1253\n",
      "Test MAE Arousal: 0.0716, Test RMSE Arousal: 0.0932\n"
     ]
    }
   ],
   "source": [
    "#GRU avg pool WITH GPU CPU SWITCH, sewa 15 //TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 15\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Directly average across the sequence length dimension\n",
    "        out = torch.mean(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_AVG-15.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7636f4e3-3beb-4d9e-98dd-beecef674cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1436\n",
      "Epoch 1/100, Validation Loss: 0.1153\n",
      "Model saved to sewa-best_GRU_AVG-20.pth\n",
      "Epoch 2/100, Training Loss: 0.1109\n",
      "Epoch 2/100, Validation Loss: 0.0895\n",
      "Model saved to sewa-best_GRU_AVG-20.pth\n",
      "Epoch 3/100, Training Loss: 0.1055\n",
      "Epoch 3/100, Validation Loss: 0.1042\n",
      "Epoch 4/100, Training Loss: 0.1025\n",
      "Epoch 4/100, Validation Loss: 0.1000\n",
      "Epoch 5/100, Training Loss: 0.0957\n",
      "Epoch 5/100, Validation Loss: 0.0966\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 6/100, Training Loss: 0.0834\n",
      "Epoch 6/100, Validation Loss: 0.0909\n",
      "Epoch 7/100, Training Loss: 0.0785\n",
      "Epoch 7/100, Validation Loss: 0.0923\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0964, Test RMSE Valence: 0.1261\n",
      "Test MAE Arousal: 0.0669, Test RMSE Arousal: 0.0878\n"
     ]
    }
   ],
   "source": [
    "#GRU avg pool WITH GPU CPU SWITCH, sewa 20 //TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 20\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Directly average across the sequence length dimension\n",
    "        out = torch.mean(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_AVG-20.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5098374a-30b8-4a3c-88ef-6b0c58c8e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1378\n",
      "Epoch 1/100, Validation Loss: 0.0997\n",
      "Model saved to sewa-best_GRU_MAX-5.pth\n",
      "Epoch 2/100, Training Loss: 0.1246\n",
      "Epoch 2/100, Validation Loss: 0.1093\n",
      "Epoch 3/100, Training Loss: 0.1187\n",
      "Epoch 3/100, Validation Loss: 0.1075\n",
      "Epoch 4/100, Training Loss: 0.1143\n",
      "Epoch 4/100, Validation Loss: 0.1090\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.1050\n",
      "Epoch 5/100, Validation Loss: 0.1012\n",
      "Epoch 6/100, Training Loss: 0.1009\n",
      "Epoch 6/100, Validation Loss: 0.1016\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0811, Test RMSE Valence: 0.1111\n",
      "Test MAE Arousal: 0.0761, Test RMSE Arousal: 0.0990\n"
     ]
    }
   ],
   "source": [
    "#GRU max pool with GPU SWITCH, sewa 5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 5\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.5):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # torch.max returns both the max values and the indices, so we select the values with [0]\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_MAX-5.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move inputs, labels to the same device as the model\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f9e3e2-eaec-4ee2-858c-012277588dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1375\n",
      "Epoch 1/100, Validation Loss: 0.1063\n",
      "Model saved to sewa-best_GRU_MAX-10.pth\n",
      "Epoch 2/100, Training Loss: 0.1172\n",
      "Epoch 2/100, Validation Loss: 0.0966\n",
      "Model saved to sewa-best_GRU_MAX-10.pth\n",
      "Epoch 3/100, Training Loss: 0.1145\n",
      "Epoch 3/100, Validation Loss: 0.1110\n",
      "Epoch 4/100, Training Loss: 0.1109\n",
      "Epoch 4/100, Validation Loss: 0.1141\n",
      "Epoch 5/100, Training Loss: 0.1064\n",
      "Epoch 5/100, Validation Loss: 0.1034\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 6/100, Training Loss: 0.0963\n",
      "Epoch 6/100, Validation Loss: 0.0995\n",
      "Epoch 7/100, Training Loss: 0.0920\n",
      "Epoch 7/100, Validation Loss: 0.1009\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0788, Test RMSE Valence: 0.1157\n",
      "Test MAE Arousal: 0.0630, Test RMSE Arousal: 0.0780\n"
     ]
    }
   ],
   "source": [
    "#GRU max pool with GPU SWITCH, sewa 10  //TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 10\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.5):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # torch.max returns both the max values and the indices, so we select the values with [0]\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_MAX-10.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move inputs, labels to the same device as the model\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc792d0e-eb79-4ad4-bcdd-6d98a5f1123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1387\n",
      "Epoch 1/100, Validation Loss: 0.1028\n",
      "Model saved to sewa-best_GRU_MAX-15.pth\n",
      "Epoch 2/100, Training Loss: 0.1135\n",
      "Epoch 2/100, Validation Loss: 0.0899\n",
      "Model saved to sewa-best_GRU_MAX-15.pth\n",
      "Epoch 3/100, Training Loss: 0.1119\n",
      "Epoch 3/100, Validation Loss: 0.1177\n",
      "Epoch 4/100, Training Loss: 0.1116\n",
      "Epoch 4/100, Validation Loss: 0.1063\n",
      "Epoch 5/100, Training Loss: 0.1048\n",
      "Epoch 5/100, Validation Loss: 0.0945\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 6/100, Training Loss: 0.0953\n",
      "Epoch 6/100, Validation Loss: 0.0947\n",
      "Epoch 7/100, Training Loss: 0.0895\n",
      "Epoch 7/100, Validation Loss: 0.0954\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0789, Test RMSE Valence: 0.1123\n",
      "Test MAE Arousal: 0.0654, Test RMSE Arousal: 0.0834\n"
     ]
    }
   ],
   "source": [
    "#GRU max pool with GPU SWITCH, sewa 15  //TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 15\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.5):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # torch.max returns both the max values and the indices, so we select the values with [0]\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_MAX-15.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move inputs, labels to the same device as the model\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path)) \n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803a5535-3714-4fba-be1a-adb76f324da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1418\n",
      "Epoch 1/100, Validation Loss: 0.0922\n",
      "Model saved to sewa-best_GRU_MAX-20.pth\n",
      "Epoch 2/100, Training Loss: 0.1143\n",
      "Epoch 2/100, Validation Loss: 0.1093\n",
      "Epoch 3/100, Training Loss: 0.1102\n",
      "Epoch 3/100, Validation Loss: 0.0998\n",
      "Epoch 4/100, Training Loss: 0.1069\n",
      "Epoch 4/100, Validation Loss: 0.1100\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.0917\n",
      "Epoch 5/100, Validation Loss: 0.0943\n",
      "Epoch 6/100, Training Loss: 0.0876\n",
      "Epoch 6/100, Validation Loss: 0.0929\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.0742, Test RMSE Valence: 0.1063\n",
      "Test MAE Arousal: 0.0686, Test RMSE Arousal: 0.0861\n"
     ]
    }
   ],
   "source": [
    "#GRU max pool with GPU SWITCH, sewa 20  //TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU before converting to numpy\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 20\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # torch.max returns both the max values and the indices, so we select the values with [0]\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_MAX-20.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move inputs, labels to the same device as the model\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1596520-0a6f-4bc4-9971-d56f7c867d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.9033\n",
      "Epoch 1/100, Validation Loss: 1.0495\n",
      "Model saved to sewa-best_GRU_1D-5.pth\n",
      "Epoch 2/100, Training Loss: 0.9860\n",
      "Epoch 2/100, Validation Loss: 1.0495\n",
      "Epoch 3/100, Training Loss: 0.9858\n",
      "Epoch 3/100, Validation Loss: 1.0495\n",
      "Epoch 4/100, Training Loss: 0.9856\n",
      "Epoch 4/100, Validation Loss: 1.0495\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.9855\n",
      "Epoch 5/100, Validation Loss: 1.0495\n",
      "Epoch 6/100, Training Loss: 0.9856\n",
      "Epoch 6/100, Validation Loss: 1.0495\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.7753, Test RMSE Valence: 0.8232\n",
      "Test MAE Arousal: 1.1981, Test RMSE Arousal: 1.2281\n"
     ]
    }
   ],
   "source": [
    "#GRU 1d cnn SEWA 5 DONE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "# Hyperparameters\n",
    "window_size = 5\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2, cnn_kernel_size=window_size, cnn_out_channels=hidden_size):\n",
    "        #cnn kernel size = window size\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        # 1D CNN Layer for local feature extraction\n",
    "        # Adjust in_channels to match the GRU's output hidden size\n",
    "        # You can choose cnn_out_channels to transform feature dimensionality if desired\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=hidden_size, out_channels=cnn_out_channels, kernel_size=cnn_kernel_size)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(cnn_out_channels, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Apply 1D CNN\n",
    "        out = out.permute(0, 2, 1)  # Permute for Conv1d\n",
    "        out = self.conv1d_layer(out)\n",
    "        out = out.squeeze()  # Squeeze the singleton dimension/ should be 128, что за входные данные? какой х\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_1D-5.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4523c9-f500-455b-ba90-2021a5dfd5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.9803\n",
      "Epoch 1/100, Validation Loss: 1.0453\n",
      "Model saved to sewa-best_GRU_1D-10.pth\n",
      "Epoch 2/100, Training Loss: 0.9872\n",
      "Epoch 2/100, Validation Loss: 1.0453\n",
      "Epoch 3/100, Training Loss: 0.9871\n",
      "Epoch 3/100, Validation Loss: 1.0453\n",
      "Epoch 4/100, Training Loss: 0.9871\n",
      "Epoch 4/100, Validation Loss: 1.0453\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.9871\n",
      "Epoch 5/100, Validation Loss: 1.0453\n",
      "Epoch 6/100, Training Loss: 0.9872\n",
      "Epoch 6/100, Validation Loss: 1.0453\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.7735, Test RMSE Valence: 0.8217\n",
      "Test MAE Arousal: 1.2006, Test RMSE Arousal: 1.2304\n"
     ]
    }
   ],
   "source": [
    "#GRU 1d cnn SEWA 10 TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 10\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2, cnn_kernel_size=window_size, cnn_out_channels=hidden_size):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        # 1D CNN Layer for local feature extraction\n",
    "        # Adjust in_channels to match the GRU's output hidden size\n",
    "        # You can choose cnn_out_channels to transform feature dimensionality if desired\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=hidden_size, out_channels=cnn_out_channels, kernel_size=cnn_kernel_size)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(cnn_out_channels, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Apply 1D CNN\n",
    "        out = out.permute(0, 2, 1)  # Permute for Conv1d\n",
    "        out = self.conv1d_layer(out)\n",
    "        out = out.squeeze()  # Squeeze the singleton dimension\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_1D-10.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58a58a2-1600-4439-a7aa-3672c0afd17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.8628\n",
      "Epoch 1/100, Validation Loss: 0.8044\n",
      "Model saved to sewa-best_GRU_1D-15.pth\n",
      "Epoch 2/100, Training Loss: 0.8678\n",
      "Epoch 2/100, Validation Loss: 0.8044\n",
      "Epoch 3/100, Training Loss: 0.8662\n",
      "Epoch 3/100, Validation Loss: 0.8044\n",
      "Epoch 4/100, Training Loss: 0.8669\n",
      "Epoch 4/100, Validation Loss: 0.8044\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 0.8650\n",
      "Epoch 5/100, Validation Loss: 0.8044\n",
      "Epoch 6/100, Training Loss: 0.8675\n",
      "Epoch 6/100, Validation Loss: 0.8044\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 0.7720, Test RMSE Valence: 0.8206\n",
      "Test MAE Arousal: 0.7973, Test RMSE Arousal: 0.8413\n"
     ]
    }
   ],
   "source": [
    "#GRU 1d cnn SEWA 15 TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 15\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2, cnn_kernel_size=window_size, cnn_out_channels=hidden_size):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        # 1D CNN Layer for local feature extraction\n",
    "        # Adjust in_channels to match the GRU's output hidden size\n",
    "        # You can choose cnn_out_channels to transform feature dimensionality if desired\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=hidden_size, out_channels=cnn_out_channels, kernel_size=cnn_kernel_size)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(cnn_out_channels, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Apply 1D CNN\n",
    "        out = out.permute(0, 2, 1)  # Permute for Conv1d\n",
    "        out = self.conv1d_layer(out)\n",
    "        out = out.squeeze()  # Squeeze the singleton dimension\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_1D-15.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0362bb29-e1e6-4e8d-bd91-b43c25b2532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.0897\n",
      "Epoch 1/100, Validation Loss: 1.0837\n",
      "Model saved to sewa-best_GRU_1D-20.pth\n",
      "Epoch 2/100, Training Loss: 1.1013\n",
      "Epoch 2/100, Validation Loss: 1.0837\n",
      "Epoch 3/100, Training Loss: 1.1013\n",
      "Epoch 3/100, Validation Loss: 1.0837\n",
      "Epoch 4/100, Training Loss: 1.1013\n",
      "Epoch 4/100, Validation Loss: 1.0837\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5/100, Training Loss: 1.1014\n",
      "Epoch 5/100, Validation Loss: 1.0837\n",
      "Epoch 6/100, Training Loss: 1.1014\n",
      "Epoch 6/100, Validation Loss: 1.0837\n",
      "Early stopping triggered\n",
      "Test MAE Valence: 1.2307, Test RMSE Valence: 1.2620\n",
      "Test MAE Arousal: 0.7949, Test RMSE Arousal: 0.8387\n"
     ]
    }
   ],
   "source": [
    "#GRU 1d cnn SEWA 20 TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from torchsummary import summary\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu')  # Move outputs back to CPU\n",
    "\n",
    "            # Ensure labels are on CPU before converting to numpy\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    mae_valence = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    rmse_valence = sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    mae_arousal = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "\n",
    "\n",
    "def predict_on_dev(model, dev_loader):\n",
    "    y_valence_true = []\n",
    "    y_valence_pred = []\n",
    "    y_arousal_true = []\n",
    "    y_arousal_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dev_loader:\n",
    "          # Send inputs and labels to GPU\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          labels_valence = labels[:, 0]\n",
    "          labels_arousal = labels[:, 1]\n",
    "          outputs_valence = outputs[:, 0]\n",
    "          outputs_arousal = outputs[:, 1]\n",
    "\n",
    "          y_valence_true.extend(labels_valence.cpu().numpy())\n",
    "          y_valence_pred.extend(outputs_valence.cpu().numpy())\n",
    "          y_arousal_true.extend(labels_arousal.cpu().numpy())\n",
    "          y_arousal_pred.extend(outputs_arousal.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae_valence = mean_absolute_error(y_valence_true, y_valence_pred)\n",
    "    rmse_valence = sqrt(mean_squared_error(y_valence_true, y_valence_pred))\n",
    "    mae_arousal = mean_absolute_error(y_arousal_true, y_arousal_pred)\n",
    "    rmse_arousal = sqrt(mean_squared_error(y_arousal_true, y_arousal_pred))\n",
    "\n",
    "    return (mae_valence, rmse_valence, mae_arousal, rmse_arousal)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10, stride=5):\n",
    "        self.df = df\n",
    "        self.df['arousal'] = self.df['arousal'] / 10.\n",
    "        self.df['valence'] = self.df['valence'] / 10.\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.video_windows, self.labels_windows = self.prepare_windows()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_frames = self.video_windows[idx]\n",
    "        embeddings = [self.df.loc[self.df['path'] == frame, self.df.columns[3:]].values for frame in window_frames]\n",
    "        frames_tensor = torch.tensor(embeddings, dtype=torch.float32).squeeze(1)\n",
    "\n",
    "        labels = self.labels_windows[idx]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, labels_tensor\n",
    "\n",
    "    def prepare_windows(self):\n",
    "        video_frames = {}\n",
    "        labels = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            video_id = self.extract_video_info(row['path'])\n",
    "            if video_id not in video_frames:\n",
    "                video_frames[video_id] = []\n",
    "                labels[video_id] = []\n",
    "            video_frames[video_id].append(row['path'])\n",
    "            labels[video_id].append((row['arousal'], row['valence']))\n",
    "\n",
    "        video_windows = []\n",
    "        labels_windows = []\n",
    "        for video_id in video_frames:\n",
    "            frames = video_frames[video_id]\n",
    "            label_vals = labels[video_id]\n",
    "            for i in range(0, len(frames) - self.window_size + 1, self.stride):\n",
    "                video_windows.append(frames[i:i + self.window_size])\n",
    "                window_labels = label_vals[i:i + self.window_size]\n",
    "                avg_arousal = sum([label[0] for label in window_labels]) / len(window_labels)\n",
    "                avg_valence = sum([label[1] for label in window_labels]) / len(window_labels)\n",
    "                labels_windows.append((avg_arousal, avg_valence))\n",
    "\n",
    "        return video_windows, labels_windows\n",
    "\n",
    "    def extract_video_info(self, file_path):\n",
    "        parts = file_path.split('/')\n",
    "        video_id = parts[-2]\n",
    "        return video_id\n",
    "        \n",
    "# RMSELoss as a class\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "        \n",
    "# Load data\n",
    "train_df = pd.read_csv('AFEW-VA_radiant_fog_160_train.csv')\n",
    "dev_df = pd.read_csv('AFEW-VA_radiant_fog_160_dev.csv')\n",
    "test_df = pd.read_csv('AFEW-VA_radiant_fog_160_test.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 20\n",
    "input_size = 256  # Number of features (embeddings) per frame\n",
    "hidden_size = 128  # Number of features in hidden state of GRU\n",
    "output_size = 2  # Output size (arousal and valence)\n",
    "num_layers = 2  # Number of layers\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomVideoDataset(train_df, window_size)\n",
    "dev_dataset = CustomVideoDataset(dev_df, window_size)\n",
    "test_dataset = CustomVideoDataset(test_df, window_size)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2, cnn_kernel_size=window_size, cnn_out_channels=hidden_size):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        # 1D CNN Layer for local feature extraction\n",
    "        # Adjust in_channels to match the GRU's output hidden size\n",
    "        # You can choose cnn_out_channels to transform feature dimensionality if desired\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=hidden_size, out_channels=cnn_out_channels, kernel_size=cnn_kernel_size)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(cnn_out_channels, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer applied to the output of the GRU layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Apply 1D CNN\n",
    "        out = out.permute(0, 2, 1)  # Permute for Conv1d\n",
    "        out = self.conv1d_layer(out)\n",
    "        out = out.squeeze()  # Squeeze the singleton dimension\n",
    "        \n",
    "        # Apply dropout to the outputs of the GRU layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the averaged output\n",
    "        out = self.fc(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "# Initialize the model, optimizer, and RMSELoss\n",
    "model = GRUNetwork(input_size, hidden_size, output_size, num_layers, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = RMSELoss() \n",
    "\n",
    "# adding learning rate scheduler to dynamically adjust the LR\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = 'sewa-best_GRU_1D-20.pth'  # Define model save path \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mae_valence, rmse_valence, mae_arousal, rmse_arousal = evaluate_model(model, test_loader)\n",
    "print(f\"Test MAE Valence: {mae_valence:.4f}, Test RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"Test MAE Arousal: {mae_arousal:.4f}, Test RMSE Arousal: {rmse_arousal:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
